{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KhW-65JleJtR",
        "EnlAJvXxGNJX",
        "ORZLrKJ0rIX3",
        "kzof80tqtNJu",
        "K_FACWHVvPg4",
        "pTT0lrM7xDn3",
        "AWmMUEq40Gyc",
        "vIinQK8I1LVI",
        "f3a4N8Wq6R9s",
        "6bkGjUVs51ul",
        "Rji8DcKhGa_B",
        "rgdAcyxHG3AO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Optimization with PyTorch\n",
        "\n",
        "- See: https://pytorch.org/tutorials/index.html"
      ],
      "metadata": {
        "id": "QF1FhZlfd4Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning with Ray Tune\n",
        "\n",
        "- From: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
        "- Ray Tune Docs: https://docs.ray.io/en/latest/tune/index.html\n",
        "- Note: Tutorial is based on an original CNN tutorial. The comments marked by \"HPT\" show the new changes/additions with the hyperparameter tuning tutorial"
      ],
      "metadata": {
        "id": "KhW-65JleJtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ray Tune is an open source library for hyperparameter tuning. It contains the latest search algorithms, integrates with TensorBoard and other analysis libraries, and supports distributed training."
      ],
      "metadata": {
        "id": "6HPXB-1dhb-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR10 Classifier"
      ],
      "metadata": {
        "id": "H8UrYsHTiOSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "%pip install ray[tune]\n",
        "%pip install ray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPpqy_Dku3jY",
        "outputId": "e0a0f298-a1f1-4b85-c0bc-d6ec3ad26e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.0.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n",
            "Requirement already satisfied: ray in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# HPT\n",
        "from functools import partial\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from torch.utils.data import random_split\n",
        "from ray import tune\n",
        "from ray import train\n",
        "from ray.train import Checkpoint, get_checkpoint\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import ray.cloudpickle as pickle"
      ],
      "metadata": {
        "id": "z9EV-_Yyd8lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Wrap the data loaders and pass a directory to access data from different trials\n",
        "\n",
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] # Normalize from [0, 1] to [-1, 1]\n",
        "    )\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    return trainset, testset"
      ],
      "metadata": {
        "id": "mTp46WZBib0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "qECbDdMRwOVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.3 # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tduRT8dVl9bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Make parameters of the model (l1, l2) configurable to allow for tuning\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super().__init__()\n",
        "        # input is 3 x 32 x 32\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, len(classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3 x 32 x 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # 6 x 14 x 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # 16 x 5 x 5\n",
        "        x = torch.flatten(x, 1) # Flatten dimensions except for batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eDoS_2eqm32O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Wrap the training function with a few\n",
        "#  - config specifies the hyperparameters to train with (l1, l2, lr, bs)\n",
        "#  - data_dir gives the dirctory to load and store data\n",
        "#  - Use the GPU and DataParallel if available\n",
        "#  - Saving training checkpoints for advanced schedulers and fault tolerance\n",
        "#  - Sending validation loss and accuracy to Ray Tune for evaluation\n",
        "\n",
        "def train_cifar(config, data_dir=None):\n",
        "    net = Net(config[\"l1\"], config[\"l2\"])\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n",
        "\n",
        "\n",
        "    checkpoint = get_checkpoint()\n",
        "    if checkpoint:\n",
        "        with checkpoint.as_directory() as checkpoint_dir:\n",
        "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "            with open(data_path, \"rb\") as fp:\n",
        "                checkpoint_state = pickle.laod(fp)\n",
        "            start_epoch = checkpoint_state[\"epoch\"]\n",
        "            net.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
        "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs]\n",
        "    )\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=int(config[\"batch_size\"]), shuffle=True, num_workers=8\n",
        "    )\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, 10):\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:\n",
        "                print(f\"[{epoch+1}, {i + 1:5d}] loss: {running_loss / epoch_steps:.3f}\")\n",
        "                running_loss = 0.0\n",
        "                epoch_steps = 0\n",
        "\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, (inputs, labels) in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                _, predicted = torch.max(outputs.detach(), 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "\n",
        "\n",
        "        checkpoint_data = {\n",
        "            \"epoch\" : epoch,\n",
        "            \"net_state_dict\": net.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        }\n",
        "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
        "            data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "            with open(data_path, \"wb\") as fp:\n",
        "                pickle.dump(checkpoint_data, fp)\n",
        "\n",
        "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
        "            train.report(\n",
        "                {\"loss\": val_loss / val_steps, \"accuracy\": correct / total},\n",
        "                checkpoint=checkpoint,\n",
        "            )\n",
        "\n",
        "    print(\"Finished Training\")"
      ],
      "metadata": {
        "id": "wTDOqyjIxP-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Wrap a test accuracy function\n",
        "\n",
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    batch_size = 4\n",
        "    trainset, testset = load_data()\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "    with torch.no_grad():\n",
        "        for (images, labels) in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.detach(), 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            for label, prediction in zip(labels, predicted):\n",
        "                total_pred[classes[label]] += 1\n",
        "                if label == prediction:\n",
        "                    correct_pred[classes[label]] += 1\n",
        "\n",
        "        dataiter = iter(testloader)\n",
        "        images, labels = next(dataiter)\n",
        "\n",
        "        imshow(torchvision.utils.make_grid(images))\n",
        "        print('GroundTruth: ', ' '.join(f\"{classes[labels[j]]:5s}\" for j in range(batch_size)))\n",
        "\n",
        "        outputs = net(images.to(device))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        print('Predicted: ', ' '.join(f\"{classes[predicted[j]]:5s}\" for j in range(batch_size)))\n",
        "\n",
        "\n",
        "    return correct / total, correct_pred, total_pred"
      ],
      "metadata": {
        "id": "S1tyIzF4xvHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Wrap the tune functionality\n",
        "# - Setup the config parameters\n",
        "# - Use the ASHAScheduler, which terminates bad performing trials early\n",
        "# - Specify the available CPU and GPU (can be a fraction)\n",
        "\n",
        "def tune_model(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "\n",
        "    config = {\n",
        "        \"l1\": tune.choice([2 ** i for i in range(4, 9)]),\n",
        "        \"l2\": tune.choice([2 ** i for i in range(4, 9)]),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2 ** i for i in range(1, 5)])\n",
        "    }\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2,\n",
        "    )\n",
        "\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "ptOXlf03-g-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check devices\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    print(f\"cuda device count: {torch.cuda.device_count()}\")\n",
        "print(f\"Device: {device}\")\n",
        "gpus_per_trial = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BRFofaTLOxd",
        "outputId": "8612be67-674a-4fa8-e8d8-fd43fc686277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda device count: 1\n",
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HPT\n",
        "# Run the optimization\n",
        "result = tune_model(num_samples=2, max_num_epochs=5, gpus_per_trial=gpus_per_trial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMLz6MqNAEdZ",
        "outputId": "0c8ceeda-b127-486f-d180-3c5cd7aeabc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-05-09 22:33:55,793\tINFO worker.py:1749 -- Started a local Ray instance.\n",
            "2024-05-09 22:33:59,598\tINFO tune.py:253 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------+\n",
            "| Configuration for experiment     train_cifar_2024-05-09_22-33-59   |\n",
            "+--------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator             |\n",
            "| Scheduler                        AsyncHyperBandScheduler           |\n",
            "| Number of trials                 2                                 |\n",
            "+--------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/train_cifar_2024-05-09_22-33-59\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2024-05-09_22-33-50_542278_20740/artifacts/2024-05-09_22-33-59/train_cifar_2024-05-09_22-33-59/driver_artifacts`\n",
            "\n",
            "Trial status: 2 PENDING\n",
            "Current time: 2024-05-09 22:34:00. Total running time: 0s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size |\n",
            "+------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   PENDING     256     32   0.00237311             16 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2 |\n",
            "+------------------------------------------------------------------------------+\n",
            "\n",
            "Trial train_cifar_3abcf_00000 started with configuration:\n",
            "+--------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 config             |\n",
            "+--------------------------------------------------+\n",
            "| batch_size                                    16 |\n",
            "| l1                                           256 |\n",
            "| l2                                            32 |\n",
            "| lr                                       0.00237 |\n",
            "+--------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=21293)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=21293)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=21293)\u001b[0m /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "\u001b[36m(func pid=21293)\u001b[0m   self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 1 PENDING\n",
            "Current time: 2024-05-09 22:34:30. Total running time: 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size |\n",
            "+------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   RUNNING     256     32   0.00237311             16 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2 |\n",
            "+------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m [1,  2000] loss: 1.941\n",
            "\n",
            "Trial train_cifar_3abcf_00000 finished iteration 1 at 2024-05-09 22:34:49. Total running time: 49s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000000 |\n",
            "| time_this_iter_s                                  29.76733 |\n",
            "| time_total_s                                      29.76733 |\n",
            "| training_iteration                                       1 |\n",
            "| accuracy                                            0.4295 |\n",
            "| loss                                               1.57173 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00000 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 1 PENDING\n",
            "Current time: 2024-05-09 22:35:00. Total running time: 1min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   RUNNING     256     32   0.00237311             16        1            29.7673   1.57173       0.4295 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m [2,  2000] loss: 1.489\n",
            "\n",
            "Trial train_cifar_3abcf_00000 finished iteration 2 at 2024-05-09 22:35:17. Total running time: 1min 17s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000001 |\n",
            "| time_this_iter_s                                   28.2054 |\n",
            "| time_total_s                                      57.97273 |\n",
            "| training_iteration                                       2 |\n",
            "| accuracy                                            0.5189 |\n",
            "| loss                                               1.34071 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00000 saved a checkpoint for iteration 2 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 1 PENDING\n",
            "Current time: 2024-05-09 22:35:30. Total running time: 1min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   RUNNING     256     32   0.00237311             16        2            57.9727   1.34071       0.5189 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m [3,  2000] loss: 1.310\n",
            "\n",
            "Trial train_cifar_3abcf_00000 finished iteration 3 at 2024-05-09 22:35:48. Total running time: 1min 48s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000002 |\n",
            "| time_this_iter_s                                  30.71256 |\n",
            "| time_total_s                                      88.68529 |\n",
            "| training_iteration                                       3 |\n",
            "| accuracy                                            0.5498 |\n",
            "| loss                                               1.27938 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00000 saved a checkpoint for iteration 3 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000002)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 1 PENDING\n",
            "Current time: 2024-05-09 22:36:00. Total running time: 2min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   RUNNING     256     32   0.00237311             16        3            88.6853   1.27938       0.5498 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m [4,  2000] loss: 1.188\n",
            "\n",
            "Trial train_cifar_3abcf_00000 finished iteration 4 at 2024-05-09 22:36:17. Total running time: 2min 17s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000003 |\n",
            "| time_this_iter_s                                  29.28486 |\n",
            "| time_total_s                                     117.97015 |\n",
            "| training_iteration                                       4 |\n",
            "| accuracy                                            0.6031 |\n",
            "| loss                                               1.14351 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00000 saved a checkpoint for iteration 4 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 RUNNING | 1 PENDING\n",
            "Current time: 2024-05-09 22:36:30. Total running time: 2min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status       l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   RUNNING     256     32   0.00237311             16        4             117.97   1.14351       0.6031 |\n",
            "| train_cifar_3abcf_00001   PENDING      32    256   0.0042963               2                                                    |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=21293)\u001b[0m [5,  2000] loss: 1.090\n",
            "\n",
            "Trial train_cifar_3abcf_00000 finished iteration 5 at 2024-05-09 22:36:46. Total running time: 2min 46s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00000 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000004 |\n",
            "| time_this_iter_s                                  28.95614 |\n",
            "| time_total_s                                      146.9263 |\n",
            "| training_iteration                                       5 |\n",
            "| accuracy                                            0.6176 |\n",
            "| loss                                               1.10352 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00000 saved a checkpoint for iteration 5 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000004\n",
            "\n",
            "Trial train_cifar_3abcf_00000 completed after 5 iterations at 2024-05-09 22:36:46. Total running time: 2min 46s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=21293)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000004)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_3abcf_00001 started with configuration:\n",
            "+-------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00001 config            |\n",
            "+-------------------------------------------------+\n",
            "| batch_size                                    2 |\n",
            "| l1                                           32 |\n",
            "| l2                                          256 |\n",
            "| lr                                       0.0043 |\n",
            "+-------------------------------------------------+\n",
            "\u001b[36m(func pid=22478)\u001b[0m Files already downloaded and verified\n",
            "\u001b[36m(func pid=22478)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22478)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[36m(func pid=22478)\u001b[0m   warnings.warn(_create_warning_msg(\n",
            "\u001b[36m(func pid=22478)\u001b[0m /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "\u001b[36m(func pid=22478)\u001b[0m   self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial status: 1 TERMINATED | 1 RUNNING\n",
            "Current time: 2024-05-09 22:37:00. Total running time: 3min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00001   RUNNING        32    256   0.0042963               2                                                    |\n",
            "| train_cifar_3abcf_00000   TERMINATED    256     32   0.00237311             16        5            146.926   1.10352       0.6176 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1,  2000] loss: 2.122\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1,  4000] loss: 1.974\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1,  6000] loss: 1.946\n",
            "Trial status: 1 TERMINATED | 1 RUNNING\n",
            "Current time: 2024-05-09 22:37:30. Total running time: 3min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00001   RUNNING        32    256   0.0042963               2                                                    |\n",
            "| train_cifar_3abcf_00000   TERMINATED    256     32   0.00237311             16        5            146.926   1.10352       0.6176 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1,  8000] loss: 1.941\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 10000] loss: 1.911\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 12000] loss: 1.898\n",
            "Trial status: 1 TERMINATED | 1 RUNNING\n",
            "Current time: 2024-05-09 22:38:00. Total running time: 4min 0s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00001   RUNNING        32    256   0.0042963               2                                                    |\n",
            "| train_cifar_3abcf_00000   TERMINATED    256     32   0.00237311             16        5            146.926   1.10352       0.6176 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 14000] loss: 1.915\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 16000] loss: 1.917\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 18000] loss: 1.918\n",
            "Trial status: 1 TERMINATED | 1 RUNNING\n",
            "Current time: 2024-05-09 22:38:31. Total running time: 4min 30s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00001   RUNNING        32    256   0.0042963               2                                                    |\n",
            "| train_cifar_3abcf_00000   TERMINATED    256     32   0.00237311             16        5            146.926   1.10352       0.6176 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\u001b[36m(func pid=22478)\u001b[0m [1, 20000] loss: 1.925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(func pid=22478)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00001_1_batch_size=2,l1=32,l2=256,lr=0.0043_2024-05-09_22-34-00/checkpoint_000000)\n",
            "2024-05-09 22:38:54,384\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
            "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
            "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
            "2024-05-09 22:38:54,389\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/train_cifar_2024-05-09_22-33-59' in 0.0093s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial train_cifar_3abcf_00001 finished iteration 1 at 2024-05-09 22:38:54. Total running time: 4min 54s\n",
            "+------------------------------------------------------------+\n",
            "| Trial train_cifar_3abcf_00001 result                       |\n",
            "+------------------------------------------------------------+\n",
            "| checkpoint_dir_name                      checkpoint_000000 |\n",
            "| time_this_iter_s                                 121.68798 |\n",
            "| time_total_s                                     121.68798 |\n",
            "| training_iteration                                       1 |\n",
            "| accuracy                                            0.2674 |\n",
            "| loss                                               1.96072 |\n",
            "+------------------------------------------------------------+\n",
            "Trial train_cifar_3abcf_00001 saved a checkpoint for iteration 1 at: (local)/root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00001_1_batch_size=2,l1=32,l2=256,lr=0.0043_2024-05-09_22-34-00/checkpoint_000000\n",
            "\n",
            "Trial train_cifar_3abcf_00001 completed after 1 iterations at 2024-05-09 22:38:54. Total running time: 4min 54s\n",
            "\n",
            "Trial status: 2 TERMINATED\n",
            "Current time: 2024-05-09 22:38:54. Total running time: 4min 54s\n",
            "Logical resource usage: 2.0/2 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                status         l1     l2           lr     batch_size     iter     total time (s)      loss     accuracy |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "| train_cifar_3abcf_00000   TERMINATED    256     32   0.00237311             16        5            146.926   1.10352       0.6176 |\n",
            "| train_cifar_3abcf_00001   TERMINATED     32    256   0.0042963               2        1            121.688   1.96072       0.2674 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results of the best model\n",
        "\n",
        "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "print(f\"Best trial config: {best_trial.config}\")\n",
        "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n",
        "print(f\"Best trial final validation accuracy: {best_trial.last_result['accuracy']}\")\n",
        "\n",
        "best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda:0\"\n",
        "    if gpus_per_trial > 1:\n",
        "        best_trained_model = nn.DataParallel(best_trained_model)\n",
        "best_trained_model.to(device)\n",
        "\n",
        "best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric=\"accuracy\", mode=\"max\")\n",
        "with best_checkpoint.as_directory() as checkpoint_dir:\n",
        "    data_path = Path(checkpoint_dir) / \"data.pkl\"\n",
        "    print(\"Checkpoint Path: \", data_path)\n",
        "    with open(data_path, \"rb\") as fp:\n",
        "        best_checkpoint_data = pickle.load(fp)\n",
        "\n",
        "    best_trained_model.load_state_dict(best_checkpoint_data[\"net_state_dict\"])\n",
        "    test_acc, correct_pred, total_pred = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "    for classname, correct in correct_pred.items():\n",
        "        accuracy = correct / total_pred[classname] * 100\n",
        "        print(f'Accuracy of {classname:5s} is {accuracy:.1f} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "mNQ4xBOtLY80",
        "outputId": "60b08c4b-7d35-4dfb-ff69-b79348b8ba4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial config: {'l1': 256, 'l2': 32, 'lr': 0.002373113854804063, 'batch_size': 16}\n",
            "Best trial final validation loss: 1.1035199899673462\n",
            "Best trial final validation accuracy: 0.6176\n",
            "Checkpoint Path:  /root/ray_results/train_cifar_2024-05-09_22-33-59/train_cifar_3abcf_00000_0_batch_size=16,l1=256,l2=32,lr=0.0024_2024-05-09_22-34-00/checkpoint_000004/data.pkl\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundTruth:  cat   ship  ship  plane\n",
            "Predicted:  cat   ship  ship  plane\n",
            "Best trial test set accuracy: 0.6125\n",
            "Accuracy of plane is 69.5 %\n",
            "Accuracy of car   is 71.6 %\n",
            "Accuracy of bird  is 46.7 %\n",
            "Accuracy of cat   is 47.0 %\n",
            "Accuracy of deer  is 54.2 %\n",
            "Accuracy of dog   is 43.3 %\n",
            "Accuracy of frog  is 72.4 %\n",
            "Accuracy of horse is 69.4 %\n",
            "Accuracy of ship  is 76.1 %\n",
            "Accuracy of truck is 62.3 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the artifacts\n",
        "# /tmp/ray/*\n",
        "# /root/ray_results/*\n",
        "%rm -r /tmp/ray/*\n",
        "%rm -r /root/ray_results/*"
      ],
      "metadata": {
        "id": "JOnb-MYLeMd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Profiling\n",
        "\n",
        "- CPU and GPU time and memory\n",
        "- Profiling of NN modules and training/inference runs\n",
        "- Visualization with TensorBoard/HTA"
      ],
      "metadata": {
        "id": "EnlAJvXxGNJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Profiler\n",
        "\n",
        "- PyTorch recipe: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"
      ],
      "metadata": {
        "id": "ORZLrKJ0rIX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Profiler supports multithreaded models\n",
        "- Profiler runs in the same thread as the operation but it will also profile child operators that might run in another thread\n",
        "- Concurrently-running profilers will be scoped to their own thread to prevent mixing of results"
      ],
      "metadata": {
        "id": "PRdv9qgZ7Ss5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKeTl4YrG-1z",
        "outputId": "bc193718-fca5-4275-b213-dfac5c64fa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ],
      "metadata": {
        "id": "30EObW4ers6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18()\n",
        "inputs = torch.randn(5,3,224,224)"
      ],
      "metadata": {
        "id": "iyKRcDcAsP1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Profile CPU Execution Time\n",
        "---"
      ],
      "metadata": {
        "id": "kzof80tqtNJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ],
      "metadata": {
        "id": "WDP6L-loskP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZp2neDos4CH",
        "outputId": "dffe37b6-69ed-4fde-b2e6-86ed21519fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                  model_inference         3.60%      17.601ms       100.00%     488.312ms     488.312ms             1  \n",
            "                     aten::conv2d         0.62%       3.022ms        66.36%     324.028ms      16.201ms            20  \n",
            "                aten::convolution         0.80%       3.921ms        65.74%     321.006ms      16.050ms            20  \n",
            "               aten::_convolution         0.11%     545.000us        64.93%     317.085ms      15.854ms            20  \n",
            "         aten::mkldnn_convolution        64.09%     312.941ms        64.82%     316.540ms      15.827ms            20  \n",
            "                 aten::max_pool2d         0.00%      23.000us        11.39%      55.636ms      55.636ms             1  \n",
            "    aten::max_pool2d_with_indices        11.39%      55.613ms        11.39%      55.613ms      55.613ms             1  \n",
            "                 aten::batch_norm         0.03%     127.000us        11.30%      55.161ms       2.758ms            20  \n",
            "     aten::_batch_norm_impl_index         0.68%       3.326ms        11.27%      55.034ms       2.752ms            20  \n",
            "          aten::native_batch_norm        10.53%      51.398ms        10.58%      51.675ms       2.584ms            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 488.312ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=10)) # \"Self\" Excludes time in child operations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YBumZFUudBU",
        "outputId": "1ef8efd1-40b4-4dd9-b4e8-f9d7a1570c36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "         aten::mkldnn_convolution        64.09%     312.941ms        64.82%     316.540ms      15.827ms            20  \n",
            "    aten::max_pool2d_with_indices        11.39%      55.613ms        11.39%      55.613ms      55.613ms             1  \n",
            "          aten::native_batch_norm        10.53%      51.398ms        10.58%      51.675ms       2.584ms            20  \n",
            "                  model_inference         3.60%      17.601ms       100.00%     488.312ms     488.312ms             1  \n",
            "                       aten::add_         1.72%       8.417ms         1.72%       8.417ms     300.607us            28  \n",
            "                 aten::clamp_min_         1.33%       6.509ms         1.33%       6.509ms     382.882us            17  \n",
            "                      aten::copy_         1.10%       5.362ms         1.10%       5.362ms       2.681ms             2  \n",
            "                       aten::div_         1.08%       5.284ms         2.15%      10.502ms      10.502ms             1  \n",
            "                      aten::addmm         0.86%       4.185ms         0.93%       4.531ms       4.531ms             1  \n",
            "                aten::convolution         0.80%       3.921ms        65.74%     321.006ms      16.050ms            20  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 488.312ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10)) # Finer granularity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "001S98wcud_E",
        "outputId": "b88c78b6-4f90-4f36-e12c-5d9c0b23a7da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "                  model_inference         3.60%      17.601ms       100.00%     488.312ms     488.312ms             1                                                                                []  \n",
            "                     aten::conv2d         0.60%       2.906ms        23.95%     116.946ms     116.946ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                aten::convolution         0.73%       3.578ms        23.35%     114.040ms     114.040ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         0.06%     309.000us        22.62%     110.462ms     110.462ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "         aten::mkldnn_convolution        21.89%     106.882ms        22.56%     110.153ms     110.153ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
            "                     aten::conv2d         0.01%      30.000us        12.91%      63.064ms      15.766ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "                aten::convolution         0.02%      96.000us        12.91%      63.034ms      15.758ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
            "               aten::_convolution         0.01%      66.000us        12.89%      62.938ms      15.735ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
            "         aten::mkldnn_convolution        12.85%      62.768ms        12.88%      62.872ms      15.718ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
            "                 aten::max_pool2d         0.00%      23.000us        11.39%      55.636ms      55.636ms             1                                           [[5, 64, 112, 112], [], [], [], [], []]  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
            "Self CPU time total: 488.312ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#### Profile GPU Execution Time\n",
        "---\n"
      ],
      "metadata": {
        "id": "K_FACWHVvPg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5,3,224,224).cuda()\n",
        "with profile(activities=[\n",
        "    ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ],
      "metadata": {
        "id": "Pmu7vmSyvO33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDQuDZh1v9QS",
        "outputId": "d3ef037b-9a6c-4261-f2b0-f090ec6070a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us     262.054ms        96.36%     262.054ms     131.027ms             2  \n",
            "                                aten::cudnn_convolution        32.27%     137.159ms        60.72%     258.026ms      12.901ms       8.134ms         2.99%       8.134ms     406.700us            20  \n",
            "cudnn_infer_volta_scudnn_winograd_128x128_ldg1_ldg4_...         0.00%       0.000us         0.00%       0.000us       0.000us       5.572ms         2.05%       5.572ms     428.615us            13  \n",
            "                                 aten::cudnn_batch_norm         0.66%       2.801ms         2.64%      11.201ms     560.050us     992.000us         0.36%     992.000us      49.600us            20  \n",
            "cudnn_infer_volta_scudnn_128x64_relu_xregs_large_nn_...         0.00%       0.000us         0.00%       0.000us       0.000us     816.000us         0.30%     816.000us     408.000us             2  \n",
            "cudnn_infer_volta_scudnn_128x64_3dconv_fprop_small_n...         0.00%       0.000us         0.00%       0.000us       0.000us     626.000us         0.23%     626.000us     626.000us             1  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     538.000us         0.20%     538.000us      35.867us            15  \n",
            "void cudnn::winograd::generateWinogradTilesKernel<0,...         0.00%       0.000us         0.00%       0.000us       0.000us     529.000us         0.19%     529.000us      40.692us            13  \n",
            "cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_...         0.00%       0.000us         0.00%       0.000us       0.000us     315.000us         0.12%     315.000us     315.000us             1  \n",
            "                                       aten::clamp_min_         1.12%       4.763ms        12.41%      52.758ms       3.103ms     308.000us         0.11%     308.000us      18.118us            17  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 424.978ms\n",
            "Self CUDA time total: 271.939ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#### Profile Memory\n",
        "---"
      ],
      "metadata": {
        "id": "pTT0lrM7xDn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ],
      "metadata": {
        "id": "T9FboCgcxDTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txs87M0OyZMy",
        "outputId": "97ad124a-f4ef-4ad8-8a86-c4a1bd4d462a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference        35.59%       6.955ms        99.87%      19.518ms      19.518ms       0.000us         0.00%       9.904ms       9.904ms           0 b           0 b           0 b    -109.39 Mb             1  \n",
            "                                           aten::conv2d         0.89%     174.000us        28.27%       5.525ms     276.250us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                      aten::convolution         2.64%     515.000us        27.38%       5.351ms     267.550us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                     aten::_convolution         1.49%     292.000us        24.75%       4.836ms     241.800us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                aten::cudnn_convolution        15.16%       2.963ms        23.25%       4.544ms     227.200us       8.136ms        29.31%       8.136ms     406.800us           0 b           0 b      48.92 Mb      48.92 Mb            20  \n",
            "                                        cudaEventRecord         0.90%     176.000us         0.90%     176.000us       4.400us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "                                  cudaStreamIsCapturing         0.21%      42.000us         0.21%      42.000us       1.050us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "                                  cudaStreamGetPriority         0.17%      34.000us         0.17%      34.000us       0.850us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.06%      11.000us         0.06%      11.000us       0.275us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "                                       cudaLaunchKernel        14.03%       2.741ms        14.03%       2.741ms      24.473us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           112  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 19.543ms\n",
            "Self CUDA time total: 27.763ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSHf-sKCzgH2",
        "outputId": "aa5de0a9-e53a-4185-f185-81a102418bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::empty         5.92%       1.157ms         5.92%       1.157ms      11.570us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      48.96 Mb      48.96 Mb           100  \n",
            "                                aten::cudnn_convolution        15.16%       2.963ms        23.25%       4.544ms     227.200us       8.136ms        29.31%       8.136ms     406.800us           0 b           0 b      48.92 Mb      48.92 Mb            20  \n",
            "                          aten::max_pool2d_with_indices         0.42%      82.000us         0.57%     112.000us     112.000us     201.000us         0.72%     201.000us     201.000us           0 b           0 b      11.48 Mb      11.48 Mb             1  \n",
            "                                            aten::addmm         0.74%     145.000us         0.86%     169.000us     169.000us      27.000us         0.10%      27.000us      27.000us           0 b           0 b      20.00 Kb      20.00 Kb             1  \n",
            "                                             aten::mean         0.36%      71.000us         0.48%      94.000us      94.000us      15.000us         0.05%      15.000us      15.000us           0 b           0 b      10.00 Kb      10.00 Kb             1  \n",
            "                                           aten::conv2d         0.89%     174.000us        28.27%       5.525ms     276.250us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                      aten::convolution         2.64%     515.000us        27.38%       5.351ms     267.550us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                     aten::_convolution         1.49%     292.000us        24.75%       4.836ms     241.800us       0.000us         0.00%       8.136ms     406.800us           0 b           0 b      48.92 Mb           0 b            20  \n",
            "                                        cudaEventRecord         0.90%     176.000us         0.90%     176.000us       4.400us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "                                  cudaStreamIsCapturing         0.21%      42.000us         0.21%      42.000us       1.050us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            40  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 19.543ms\n",
            "Self CUDA time total: 27.763ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#### Tracing\n",
        "---"
      ],
      "metadata": {
        "id": "AWmMUEq40Gyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18().cuda()\n",
        "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
        "\n",
        "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "    model(inputs)\n",
        "\n",
        "prof.export_chrome_trace(\"trace.json\")\n",
        "\n",
        "# Open trace file in Chrome window at chrome://tracing"
      ],
      "metadata": {
        "id": "TsqgCjIG0I2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "#### Stack Traces\n",
        "---"
      ],
      "metadata": {
        "id": "vIinQK8I1LVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    with_stack=True,\n",
        ") as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        model(inputs)"
      ],
      "metadata": {
        "id": "HAWCe0050PNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n",
        "# Not working, may be a PyTorch issue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYitVek1Ky5",
        "outputId": "f4a16282-f06f-482d-b2c8-f42a6617d460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        model_inference         0.00%       0.000us         0.00%       0.000us       0.000us      10.519ms        51.51%      10.519ms      10.519ms             1  \n",
            "                                aten::cudnn_convolution        29.68%       4.860ms        44.70%       7.319ms     365.950us       8.139ms        39.86%       8.139ms     406.950us            20  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 16.374ms\n",
            "Self CUDA time total: 20.421ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### Long-running Jobs\n",
        "---"
      ],
      "metadata": {
        "id": "R2CHmZXX3rpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Jobs like training can take a long time and have different requirements\n",
        "- Tracing all of a long job can be very slow and output large trace files\n",
        "\n",
        "schedule - specifies a function that takes an integer argument (step number) as an input and returns an action for the profiler, the best way to use this parameter is to use torch.profiler.schedule helper function that can generate a schedule for you;\n",
        "\n",
        "on_trace_ready - specifies a function that takes a reference to the profiler as an input and is called by the profiler each time the new trace is ready."
      ],
      "metadata": {
        "id": "3vAVHF5l35Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.profiler import schedule\n",
        "\n",
        "my_schedule = schedule(\n",
        "    skip_first=10, # ignore first 10 steps\n",
        "    wait=5,        # Idle time before collection\n",
        "    warmup=1,      # Initial discarded traces due to extra overhead\n",
        "    active=3,      # Number of steps to record data\n",
        "    repeat=2)      # Upper bound on the number of cycles (wait,warmup,active) to collect"
      ],
      "metadata": {
        "id": "TO-Z-ozM3zKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_handler(p):\n",
        "    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n",
        "    print(output)\n",
        "    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n",
        "\n",
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(\n",
        "        wait=1,\n",
        "        warmup=1,\n",
        "        active=2),\n",
        "    on_trace_ready=trace_handler\n",
        ") as p:\n",
        "    for idx in range(8):\n",
        "        model(inputs)\n",
        "        p.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k81Lk5-W5YSF",
        "outputId": "c0402453-a283-4704-dabb-00bcdb4da08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us      20.225ms        45.53%      20.225ms      10.113ms             2  \n",
            "                                aten::cudnn_convolution         7.24%       1.779ms        10.23%       2.514ms      62.850us      16.297ms        36.68%      16.297ms     407.425us            40  \n",
            "cudnn_infer_volta_scudnn_winograd_128x128_ldg1_ldg4_...         0.00%       0.000us         0.00%       0.000us       0.000us      13.815ms        31.10%      13.815ms     445.645us            31  \n",
            "                                 aten::cudnn_batch_norm         6.58%       1.618ms        14.52%       3.570ms      89.250us       1.999ms         4.50%       1.999ms      49.975us            40  \n",
            "cudnn_infer_volta_scudnn_128x64_3dconv_fprop_small_n...         0.00%       0.000us         0.00%       0.000us       0.000us       1.880ms         4.23%       1.880ms     626.667us             3  \n",
            "cudnn_infer_volta_scudnn_128x64_relu_xregs_large_nn_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.629ms         3.67%       1.629ms     407.250us             4  \n",
            "void cudnn::winograd::generateWinogradTilesKernel<0,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.523ms         3.43%       1.523ms      49.129us            31  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us       1.428ms         3.21%       1.428ms      36.615us            39  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     652.000us         1.47%     652.000us      15.902us            41  \n",
            "cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_...         0.00%       0.000us         0.00%       0.000us       0.000us     630.000us         1.42%     630.000us     315.000us             2  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 24.583ms\n",
            "Self CUDA time total: 44.425ms\n",
            "\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us      20.228ms        43.39%      20.228ms      10.114ms             2  \n",
            "                                aten::cudnn_convolution         6.55%       1.755ms         9.05%       2.427ms      60.675us      16.290ms        34.94%      16.290ms     407.250us            40  \n",
            "cudnn_infer_volta_scudnn_winograd_128x128_ldg1_ldg4_...         0.00%       0.000us         0.00%       0.000us       0.000us      15.255ms        32.72%      15.255ms     435.857us            35  \n",
            "                                 aten::cudnn_batch_norm         5.97%       1.599ms        12.00%       3.216ms      80.400us       1.999ms         4.29%       1.999ms      49.975us            40  \n",
            "cudnn_infer_volta_scudnn_128x64_3dconv_fprop_small_n...         0.00%       0.000us         0.00%       0.000us       0.000us       1.880ms         4.03%       1.880ms     626.667us             3  \n",
            "cudnn_infer_volta_scudnn_128x64_relu_xregs_large_nn_...         0.00%       0.000us         0.00%       0.000us       0.000us       1.631ms         3.50%       1.631ms     407.750us             4  \n",
            "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us       1.610ms         3.45%       1.610ms      35.778us            45  \n",
            "void cudnn::winograd::generateWinogradTilesKernel<0,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.583ms         3.40%       1.583ms      45.229us            35  \n",
            "cudnn_infer_volta_scudnn_128x32_sliced1x4_ldg4_relu_...         0.00%       0.000us         0.00%       0.000us       0.000us     943.000us         2.02%     943.000us     314.333us             3  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     697.000us         1.50%     697.000us      15.152us            46  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 26.805ms\n",
            "Self CUDA time total: 46.619ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Profile Modules\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/beginner/profiler.html"
      ],
      "metadata": {
        "id": "f3a4N8Wq6R9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.autograd.profiler as profiler"
      ],
      "metadata": {
        "id": "qZ7ITzrV6fTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        with profiler.record_function(\"Linear\"):\n",
        "            out = self.linear(input)\n",
        "\n",
        "        with profiler.record_function(\"Mask\"):\n",
        "            threshold = out.sum(axis=1).mean().item()\n",
        "            hi_idx = np.argwhere(mask.cpu().numpy() > threshold) # Copy to CPU\n",
        "            hi_idx = torch.from_numpy(hi_idx).cuda() # Copy to CUDA\n",
        "\n",
        "        return out, hi_idx"
      ],
      "metadata": {
        "id": "xvReypciL3eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128,500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()\n",
        "\n",
        "# warm up cuda\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)"
      ],
      "metadata": {
        "id": "5VgcoHUIOVUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tc0otDHQUcJ",
        "outputId": "6a2b83ec-8072-464f-b1f9-66a51b5edf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Mask        63.83%        1.529s        99.94%        2.394s        2.394s           0 b    -953.67 Mb     817.74 Mb      -1.00 Kb             1  \n",
            "                                  cudaMemcpyAsync        36.09%     864.348ms        36.09%     864.348ms     288.116ms           0 b           0 b           0 b           0 b             3  \n",
            "                                      aten::addmm         0.04%       1.035ms         0.04%       1.074ms       1.074ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                           Linear         0.01%     273.000us         0.06%       1.409ms       1.409ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "                                      aten::copy_         0.01%     125.000us        36.10%     864.525ms     432.262ms           0 b           0 b           0 b           0 b             2  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.395s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improve the memory performace of Mask by casting as float instead of double\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n",
        "\n",
        "# warm up cuda\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)"
      ],
      "metadata": {
        "id": "UCcapgEhRSC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XgOFUZNRfU8",
        "outputId": "cadf3d3e-4410-4ca9-8b20-764aa53c6351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Mask        72.76%        1.496s        99.91%        2.053s        2.053s           0 b    -476.84 Mb     817.78 Mb      -1.00 Kb             1  \n",
            "                                  cudaMemcpyAsync        27.12%     557.360ms        27.12%     557.360ms     185.787ms           0 b           0 b           0 b           0 b             3  \n",
            "                                      aten::addmm         0.08%       1.618ms         0.08%       1.663ms       1.663ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                           Linear         0.01%     169.000us         0.09%       1.895ms       1.895ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "                                      aten::copy_         0.00%      93.000us        27.12%     557.502ms     278.751ms           0 b           0 b           0 b           0 b             2  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 2.055s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improve the time performance by eliminating copies\n",
        "class MyModule(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        with profiler.record_function(\"Linear\"):\n",
        "            out = self.linear(input)\n",
        "\n",
        "        with profiler.record_function(\"Mask\"):\n",
        "            threshold = out.sum(axis=1).mean()\n",
        "            hi_idx = (mask > threshold).nonzero(as_tuple=True)\n",
        "\n",
        "        return out, hi_idx\n",
        "\n",
        "\n",
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128, 500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n",
        "\n",
        "# warm-up\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)\n",
        "\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42QMhKh7SSO-",
        "outputId": "92fd0640-fdc3-4a38-b8e9-8bdfba12ae9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        cudaMemcpyAsync        82.07%       7.260ms        82.07%       7.260ms       7.260ms           0 b           0 b           0 b           0 b             1  \n",
            "                                            aten::addmm         9.47%     838.000us         9.73%     861.000us     861.000us           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                                 Linear         3.97%     351.000us        14.11%       1.248ms       1.248ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "                                                   Mask         0.95%      84.000us        85.89%       7.598ms       7.598ms           0 b           0 b         512 b    -119.21 Mb             1  \n",
            "                                          aten::nonzero         0.83%      73.000us        83.42%       7.379ms       7.379ms           0 b           0 b           0 b           0 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 8.846ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html\n",
        "\n",
        "Also check Holistic Trace Analysis: https://hta.readthedocs.io/en/latest/index.html\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/beginner/hta_intro_tutorial.html"
      ],
      "metadata": {
        "id": "S_ODdQpw6hNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install HolisticTraceAnalysis\n",
        "%pip install torch_tb_profiler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmHeKvN-cChi",
        "outputId": "fb783433-3cef-442e-836b-90b8e52e9b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting HolisticTraceAnalysis\n",
            "  Downloading HolisticTraceAnalysis-0.2.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab>=3.5.1 (from HolisticTraceAnalysis)\n",
            "  Downloading jupyterlab-4.2.1-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from HolisticTraceAnalysis) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from HolisticTraceAnalysis) (2.0.3)\n",
            "Requirement already satisfied: plotly>=5.11.0 in /usr/local/lib/python3.10/dist-packages (from HolisticTraceAnalysis) (5.15.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Collecting httpx>=0.25.0 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel>=6.5.0 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading ipykernel-6.29.4-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-server<3,>=2.4.0 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyter_server-2.14.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-server<3,>=2.27.1 (from jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyterlab_server-2.27.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (24.0)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.0.1)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (6.3.3)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from jupyterlab>=3.5.1->HolisticTraceAnalysis) (5.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.2->HolisticTraceAnalysis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.2->HolisticTraceAnalysis) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.2->HolisticTraceAnalysis) (2024.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.11.0->HolisticTraceAnalysis) (8.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.1 (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (24.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.0.3->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.1.5)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.2.2)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (23.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyter_client-8.6.2-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.20.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.15.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.19.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.2->HolisticTraceAnalysis) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (21.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.18.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (6.0.1)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.0.7)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.7.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.8.4)\n",
            "Collecting fqdn (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.13)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab>=3.5.1->HolisticTraceAnalysis) (2.22)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab>=3.5.1->HolisticTraceAnalysis)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, jsonpointer, json5, jedi, h11, fqdn, comm, async-lru, jupyter-server-terminals, jupyter-client, httpcore, arrow, isoduration, ipykernel, httpx, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, HolisticTraceAnalysis\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.4 which is incompatible.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed HolisticTraceAnalysis-0.2.0 arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fqdn-1.5.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ipykernel-6.29.4 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-2.4 jupyter-client-8.6.2 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.1 jupyter-server-terminals-0.5.3 jupyterlab-4.2.1 jupyterlab-server-2.27.2 overrides-7.7.0 python-json-logger-2.0.7 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim\n",
        "import torch.profiler\n",
        "import torch.utils.data\n",
        "import torchvision.datasets\n",
        "import torchvision.models\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "4VDZdw2B7AQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose(\n",
        "    [T.Resize(224),\n",
        "     T.ToTensor(),\n",
        "     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v7JY7mJVmya",
        "outputId": "1c3ccacc-f847-4555-8e9e-8e842dfb75fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44689421.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda:0\")\n",
        "device = torch.device('cpu')\n",
        "model = torchvision.models.resnet18(weights='IMAGENET1K_V1').to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "model.train()\n",
        "print(\"Resnet model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1fFo8eIVwcS",
        "outputId": "c72278b3-2bfc-4ed5-e166-4196738f82d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resnet model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(data):\n",
        "    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "mCMrO-B7XshP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.profiler.profile(\n",
        "    activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name='./log/resnet18', use_gzip=False), # Generates and saves tensorboard files\n",
        "    record_shapes=True,\n",
        "    profile_memory=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    for step, batch_data in enumerate(train_loader):\n",
        "        train(batch_data)\n",
        "        prof.step()  # Need to call this at each step to notify profiler of steps' boundary.\n",
        "        if step >= 1 + (1 + 3) * 2:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCEp_xaSX2n9",
        "outputId": "d977a944-48a2-42af-aeda-9a0830c3d597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py:228: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard\n",
        "# tensorboard --logdir=./log\n",
        "# http://localhost:6006/#pytorch_profiler"
      ],
      "metadata": {
        "id": "J8OQfoxmcH81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HTA\n",
        "from hta.trace_analysis import TraceAnalysis\n",
        "trace_dir = \"/content/log/resnet18/\"\n",
        "trace_files = {0 : '8c230de2449a_145.1717189838062872263.pt.trace.json'}\n",
        "analyzer = TraceAnalysis(trace_dir=trace_dir, trace_files=trace_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iTtwfjGq0ZK",
        "outputId": "d31e9971-1f28-4607-a258-4be8f1e8dd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-31 22:01:25,443 - hta - trace.py:L389 - INFO - /content/log/resnet18/\n",
            "2024-05-31 22:01:25,446 - hta - trace.py:L535 - INFO - ranks=[0]\n",
            "2024-05-31 22:01:26,987 - hta - trace.py:L118 - INFO - Parsed /content/log/resnet18/8c230de2449a_145.1717189838062872263.pt.trace.json time = 1.54 seconds \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try HTA when GPU available\n",
        "# temporal_breakdown_df = analyzer.get_temporal_breakdown()"
      ],
      "metadata": {
        "id": "yzbef8ddrIWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmarking\n",
        "\n",
        "- Time execution of functions over many runs\n",
        "- Vary the inputs and number of threads used\n",
        "- Compare different algorithms or environments\n",
        "\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/recipes/recipes/benchmark.html"
      ],
      "metadata": {
        "id": "6bkGjUVs51ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def batched_dot_mul_sum(a, b):\n",
        "    '''Computes batched dot by multiplying and summing'''\n",
        "    return a.mul(b).sum(-1)\n",
        "\n",
        "\n",
        "def batched_dot_bmm(a, b):\n",
        "    '''Computes batched dot by reducing to ``bmm``'''\n",
        "    a = a.reshape(-1, 1, a.shape[-1])\n",
        "    b = b.reshape(-1, b.shape[-1], 1)\n",
        "    return torch.bmm(a, b).flatten(-3)\n",
        "\n",
        "\n",
        "# Input for benchmarking\n",
        "x = torch.randn(10000, 64)\n",
        "\n",
        "# Ensure that both functions compute the same output\n",
        "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
      ],
      "metadata": {
        "id": "G55p2Kxb6F-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python timeit module\n",
        "\n",
        "import timeit\n",
        "\n",
        "t0 = timeit.Timer(\n",
        "    stmt='batched_dot_mul_sum(x, x)',\n",
        "    setup='from __main__ import batched_dot_mul_sum',\n",
        "    globals={'x': x})\n",
        "\n",
        "t1 = timeit.Timer(\n",
        "    stmt='batched_dot_bmm(x, x)',\n",
        "    setup='from __main__ import batched_dot_bmm',\n",
        "    globals={'x': x})\n",
        "\n",
        "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
        "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2KfEwTCV_t7",
        "outputId": "e90acc1d-85f7-4ea8-92a5-ad05267536ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mul_sum(x, x):  1841.2 us\n",
            "bmm(x, x):      4310.3 us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PyTorch benchmark\n",
        "\n",
        "- Performs synchronization with CUDA\n",
        "- Accounts for warm-up time\n",
        "- Runs a single thread by default\n",
        "- Reports the time per run instead of the total runtime\n",
        "'''\n",
        "import torch.utils.benchmark as benchmark\n",
        "\n",
        "t0 = benchmark.Timer(\n",
        "    stmt='batched_dot_mul_sum(x, x)',\n",
        "    setup='from __main__ import batched_dot_mul_sum',\n",
        "    globals={'x': x})\n",
        "\n",
        "t1 = benchmark.Timer(\n",
        "    stmt='batched_dot_bmm(x, x)',\n",
        "    setup='from __main__ import batched_dot_bmm',\n",
        "    globals={'x': x})\n",
        "\n",
        "print(t0.timeit(100))\n",
        "print(t1.timeit(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8G58IntXP6V",
        "outputId": "7c0a5eb7-f68d-4c88-f40c-9d72cf78e2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d46a0f250>\n",
            "batched_dot_mul_sum(x, x)\n",
            "setup: from __main__ import batched_dot_mul_sum\n",
            "  653.17 us\n",
            "  1 measurement, 100 runs , 1 thread\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d46a0ff70>\n",
            "batched_dot_bmm(x, x)\n",
            "setup: from __main__ import batched_dot_bmm\n",
            "  1.78 ms\n",
            "  1 measurement, 100 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(10000, 1024, device='cuda')\n",
        "\n",
        "num_threads = torch.get_num_threads()\n",
        "print(f'Benchmarking on {num_threads} threads')\n",
        "\n",
        "t0 = benchmark.Timer(\n",
        "    stmt='batched_dot_mul_sum(x, x)',\n",
        "    setup='from __main__ import batched_dot_mul_sum',\n",
        "    globals={'x': x},\n",
        "    num_threads=num_threads,\n",
        "    label='Multithreaded batch dot',\n",
        "    sub_label='Implemented using mul and sum')\n",
        "\n",
        "t1 = benchmark.Timer(\n",
        "    stmt='batched_dot_bmm(x, x)',\n",
        "    setup='from __main__ import batched_dot_bmm',\n",
        "    globals={'x': x},\n",
        "    num_threads=num_threads,\n",
        "    label='Multithreaded batch dot',\n",
        "    sub_label='Implemented using bmm')\n",
        "\n",
        "print(t0.timeit(100))\n",
        "print(t1.timeit(100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJBV_dwX594",
        "outputId": "e8fedf97-087c-4756-f045-d5e64b0c69fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking on 1 threads\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d47876cb0>\n",
            "Multithreaded batch dot: Implemented using mul and sum\n",
            "setup: from __main__ import batched_dot_mul_sum\n",
            "  524.99 us\n",
            "  1 measurement, 100 runs , 1 thread\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d47877250>\n",
            "Multithreaded batch dot: Implemented using bmm\n",
            "setup: from __main__ import batched_dot_bmm\n",
            "  1.11 ms\n",
            "  1 measurement, 100 runs , 1 thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Autorange - Compute statistics for a period of time (min 200 ms)\n",
        "\n",
        "m0 = t0.blocked_autorange()\n",
        "m1 = t1.blocked_autorange()\n",
        "\n",
        "print(m0)\n",
        "print(m1)\n",
        "\n",
        "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\n",
        "print(f\"Median: {m0.median * 1e6:6.2f} us\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU9Ko3HTaFyw",
        "outputId": "e2f533ff-8fc7-4986-de7b-8d9f1daf5d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d46a0feb0>\n",
            "Multithreaded batch dot: Implemented using mul and sum\n",
            "setup: from __main__ import batched_dot_mul_sum\n",
            "  Median: 594.25 us\n",
            "  IQR:    261.55 us (550.56 to 812.11)\n",
            "  115 measurements, 1 runs per measurement, 1 thread\n",
            "  WARNING: Interquartile range is 44.0% of the median measurement.\n",
            "           This suggests significant environmental influence.\n",
            "<torch.utils.benchmark.utils.common.Measurement object at 0x7a7d46a0e860>\n",
            "Multithreaded batch dot: Implemented using bmm\n",
            "setup: from __main__ import batched_dot_bmm\n",
            "  Median: 1.77 ms\n",
            "  IQR:    3.20 ms (1.71 to 4.91)\n",
            "  50 measurements, 1 runs per measurement, 1 thread\n",
            "  WARNING: Interquartile range is 181.3% of the median measurement.\n",
            "           This suggests significant environmental influence.\n",
            "Mean:   1743.68 us\n",
            "Median: 594.25 us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare benchmark results - vary inputs, # of threads\n",
        "from itertools import product\n",
        "\n",
        "# Compare takes a list of measurements which we'll save in results.\n",
        "results = []\n",
        "\n",
        "sizes = [1, 64, 1024]\n",
        "for b, n in product(sizes, sizes):\n",
        "    # label and sub_label are the rows\n",
        "    # description is the column\n",
        "    label = 'Batched dot'\n",
        "    sub_label = f'[{b}, {n}]'\n",
        "    x = torch.ones((b, n))\n",
        "    for num_threads in [1, 4, 16]:\n",
        "        results.append(benchmark.Timer(\n",
        "            stmt='batched_dot_mul_sum(x, x)',\n",
        "            setup='from __main__ import batched_dot_mul_sum',\n",
        "            globals={'x': x},\n",
        "            num_threads=num_threads,\n",
        "            label=label,\n",
        "            sub_label=sub_label,\n",
        "            description='mul/sum',\n",
        "        ).blocked_autorange(min_run_time=1))\n",
        "        results.append(benchmark.Timer(\n",
        "            stmt='batched_dot_bmm(x, x)',\n",
        "            setup='from __main__ import batched_dot_bmm',\n",
        "            globals={'x': x},\n",
        "            num_threads=num_threads,\n",
        "            label=label,\n",
        "            sub_label=sub_label,\n",
        "            description='bmm',\n",
        "        ).blocked_autorange(min_run_time=1))\n",
        "\n",
        "compare = benchmark.Compare(results)\n",
        "compare.trim_significant_figures()\n",
        "compare.colorize()\n",
        "compare.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LustfCacHL7",
        "outputId": "9cd27936-ed85-43d9-8263-5982a66a06b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[------------ Batched dot ------------]\n",
            "                    |  mul/sum  |  bmm \n",
            "1 threads: ----------------------------\n",
            "      [1, 1]        |      10   |  \u001b[92m\u001b[1m  10\u001b[0m\u001b[0m\n",
            "      [1, 64]       |      11   |    20\n",
            "      [1, 1024]     |  \u001b[92m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  11\u001b[0m\u001b[0m\n",
            "      [64, 1]       |      12   |  \u001b[34m\u001b[1m  10\u001b[0m\u001b[0m\n",
            "      [64, 64]      |       9   |    17\n",
            "      [64, 1024]    |  \u001b[2m\u001b[91m    32 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 230\u001b[0m\u001b[0m\n",
            "      [1024, 1]     |       9   |    18\n",
            "      [1024, 64]    |  \u001b[31m\u001b[1m    52 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 115\u001b[0m\u001b[0m\n",
            "      [1024, 1024]  |  \u001b[31m\u001b[1m   590 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m3510\u001b[0m\u001b[0m\n",
            "4 threads: ----------------------------\n",
            "      [1, 1]        |  \u001b[92m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  10\u001b[0m\u001b[0m\n",
            "      [1, 64]       |  \u001b[34m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  10\u001b[0m\u001b[0m\n",
            "      [1, 1024]     |  \u001b[34m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  11\u001b[0m\u001b[0m\n",
            "      [64, 1]       |  \u001b[34m\u001b[1m     8 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  11\u001b[0m\u001b[0m\n",
            "      [64, 64]      |       9   |    17\n",
            "      [64, 1024]    |  \u001b[31m\u001b[1m    90 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 170\u001b[0m\u001b[0m\n",
            "      [1024, 1]     |       9   |    18\n",
            "      [1024, 64]    |  \u001b[31m\u001b[1m   100 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 110\u001b[0m\u001b[0m\n",
            "      [1024, 1024]  |  \u001b[31m\u001b[1m   630 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m2000\u001b[0m\u001b[0m\n",
            "16 threads: ---------------------------\n",
            "      [1, 1]        |  \u001b[92m\u001b[1m     7 \u001b[0m\u001b[0m  |    16\n",
            "      [1, 64]       |  \u001b[34m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  11\u001b[0m\u001b[0m\n",
            "      [1, 1024]     |  \u001b[34m\u001b[1m     7 \u001b[0m\u001b[0m  |    17\n",
            "      [64, 1]       |  \u001b[34m\u001b[1m     7 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m  11\u001b[0m\u001b[0m\n",
            "      [64, 64]      |       9   |  \u001b[2m\u001b[91m  28\u001b[0m\u001b[0m\n",
            "      [64, 1024]    |  \u001b[31m\u001b[1m   220 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 250\u001b[0m\u001b[0m\n",
            "      [1024, 1]     |       9   |  \u001b[2m\u001b[91m  32\u001b[0m\u001b[0m\n",
            "      [1024, 64]    |  \u001b[31m\u001b[1m   240 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 170\u001b[0m\u001b[0m\n",
            "      [1024, 1024]  |  \u001b[31m\u001b[1m  1000 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m4200\u001b[0m\u001b[0m\n",
            "\n",
            "Times are in microseconds (us).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and loading with pickle\n",
        "# Can do A/B test with results from different Python envs\n",
        "import pickle\n",
        "\n",
        "ab_test_results = []\n",
        "for env in ('environment A: mul/sum', 'environment B: bmm'):\n",
        "    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n",
        "        x = torch.ones((b, n))\n",
        "        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n",
        "        m = benchmark.Timer(\n",
        "            stmt='batched_dot(x, x)',\n",
        "            globals={'x': x, 'batched_dot': dot_fn},\n",
        "            num_threads=1,\n",
        "            label='Batched dot',\n",
        "            description=f'[{b}, {n}]',\n",
        "            env=env,\n",
        "        ).blocked_autorange(min_run_time=1)\n",
        "        ab_test_results.append(pickle.dumps(m))\n",
        "\n",
        "ab_results = [pickle.loads(i) for i in ab_test_results]\n",
        "compare = benchmark.Compare(ab_results)\n",
        "compare.trim_significant_figures()\n",
        "compare.colorize()\n",
        "compare.print()\n",
        "\n",
        "# Demonstrate pickle round-trip\n",
        "round_tripped_results = pickle.loads(pickle.dumps(results))\n",
        "assert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZwl2tq7dlWX",
        "outputId": "b2b6e155-82f8-4d23-a332-e1eeb4de2b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[------------------------------------- Batched dot -------------------------------------]\n",
            "                                               |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n",
            "1 threads: ------------------------------------------------------------------------------\n",
            "  (environment A: mul/sum)  batched_dot(x, x)  |  \u001b[34m\u001b[1m  10  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    31000    \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    18    \u001b[0m\u001b[0m\n",
            "  (environment B: bmm)      batched_dot(x, x)  |  \u001b[92m\u001b[1m  10  \u001b[0m\u001b[0m  |      30000      |  \u001b[31m\u001b[1m   170    \u001b[0m\u001b[0m\n",
            "\n",
            "Times are in microseconds (us).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs with fuzzed parameters - gives more variety for input tensors\n",
        "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
        "\n",
        "# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n",
        "# ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average.\n",
        "example_fuzzer = Fuzzer(\n",
        "    parameters = [\n",
        "        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n",
        "        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n",
        "    ],\n",
        "    tensors = [\n",
        "        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n",
        "    ],\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "results = []\n",
        "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
        "    # description is the column label\n",
        "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
        "    results.append(benchmark.Timer(\n",
        "        stmt='batched_dot_mul_sum(x, x)',\n",
        "        setup='from __main__ import batched_dot_mul_sum',\n",
        "        globals=tensors,\n",
        "        label='Batched dot',\n",
        "        sub_label=sub_label,\n",
        "        description='mul/sum',\n",
        "    ).blocked_autorange(min_run_time=1))\n",
        "    results.append(benchmark.Timer(\n",
        "        stmt='batched_dot_bmm(x, x)',\n",
        "        setup='from __main__ import batched_dot_bmm',\n",
        "        globals=tensors,\n",
        "        label='Batched dot',\n",
        "        sub_label=sub_label,\n",
        "        description='bmm',\n",
        "    ).blocked_autorange(min_run_time=1))\n",
        "\n",
        "compare = benchmark.Compare(results)\n",
        "compare.trim_significant_figures()\n",
        "compare.print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEE52XZufDx-",
        "outputId": "9ff755ab-b8b7-4dd0-9193-e85428a4a38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[--------------------- Batched dot ---------------------]\n",
            "                                     |  mul/sum  |   bmm \n",
            "1 threads: ----------------------------------------------\n",
            "      725    x 257                   |     150   |    500\n",
            "      49     x 383                   |      17   |     66\n",
            "      34     x 1468                  |      18   |    175\n",
            "      187    x 5039                  |     529   |   3110\n",
            "      2140   x 1296 (discontiguous)  |    2400   |  64000\n",
            "      78     x 1598                  |      43   |    421\n",
            "      519    x 763                   |     200   |   1300\n",
            "      141    x 1082                  |      71   |    610\n",
            "      78     x 5    (discontiguous)  |       8   |     15\n",
            "      187    x 1                     |       8   |     12\n",
            "\n",
            "Times are in microseconds (us).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instruction counts with Callgrind\n",
        "# More deterministic proxy for where time is spent in a process\n",
        "# Use C++ code to show the difference of passing by value or reference\n",
        "# Note: I did not run this code. Need more packages\n",
        "\n",
        "batched_dot_src = \"\"\"\\\n",
        "/* ---- Python ---- */\n",
        "// def batched_dot_mul_sum(a, b):\n",
        "//     return a.mul(b).sum(-1)\n",
        "\n",
        "torch::Tensor batched_dot_mul_sum_v0(\n",
        "    const torch::Tensor a,\n",
        "    const torch::Tensor b) {\n",
        "  return a.mul(b).sum(-1);\n",
        "}\n",
        "\n",
        "torch::Tensor batched_dot_mul_sum_v1(\n",
        "    const torch::Tensor& a,\n",
        "    const torch::Tensor& b) {\n",
        "  return a.mul(b).sum(-1);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# PyTorch makes it easy to test our C++ implementations by providing a utility\n",
        "# to JIT compile C++ source into Python extensions:\n",
        "import os\n",
        "from torch.utils import cpp_extension\n",
        "cpp_lib = cpp_extension.load_inline(\n",
        "    name='cpp_lib',\n",
        "    cpp_sources=batched_dot_src,\n",
        "    extra_cflags=['-O3'],\n",
        "    extra_include_paths=[\n",
        "        # `load_inline` needs to know where to find ``pybind11`` headers.\n",
        "        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n",
        "    ],\n",
        "    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n",
        ")\n",
        "\n",
        "# `load_inline` will create a shared object that is loaded into Python. When we collect\n",
        "# instruction counts Timer will create a subprocess, so we need to re-import it. The\n",
        "# import process is slightly more complicated for C extensions, but that's all we're\n",
        "# doing here.\n",
        "module_import_str = f\"\"\"\\\n",
        "# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\n",
        "cpp_lib = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(cpp_lib)\"\"\"\n",
        "\n",
        "import textwrap\n",
        "def pretty_print(result):\n",
        "    \"\"\"Import machinery for ``cpp_lib.so`` can get repetitive to look at.\"\"\"\n",
        "    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n",
        "\n",
        "\n",
        "t_baseline = benchmark.Timer(\n",
        "    stmt='batched_dot_mul_sum(x, x)',\n",
        "    setup='''\\\n",
        "from __main__ import batched_dot_mul_sum\n",
        "x = torch.randn(2, 2)''')\n",
        "\n",
        "t0 = benchmark.Timer(\n",
        "    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n",
        "    setup=f'''\\\n",
        "{module_import_str}\n",
        "x = torch.randn(2, 2)''')\n",
        "\n",
        "t1 = benchmark.Timer(\n",
        "    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n",
        "    setup=f'''\\\n",
        "{module_import_str}\n",
        "x = torch.randn(2, 2)''')\n",
        "\n",
        "# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n",
        "# calling convention is more efficient. v1 (call with references) seems to\n",
        "# be a bit faster, but it's within measurement error.\n",
        "pretty_print(t_baseline.blocked_autorange())\n",
        "pretty_print(t0.blocked_autorange())\n",
        "pretty_print(t1.blocked_autorange())"
      ],
      "metadata": {
        "id": "uYupb-5Nf0gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use ``Callgrind`` to determine which is better.\n",
        "stats_v0 = t0.collect_callgrind()\n",
        "stats_v1 = t1.collect_callgrind()\n",
        "\n",
        "pretty_print(stats_v0)\n",
        "pretty_print(stats_v1)\n",
        "\n",
        "# `.as_standardized` removes file names and some path prefixes, and makes\n",
        "# it easier to read the function symbols.\n",
        "stats_v0 = stats_v0.as_standardized()\n",
        "stats_v1 = stats_v1.as_standardized()\n",
        "\n",
        "# `.delta` diffs the instruction counts, and `.denoise` removes several\n",
        "# functions in the Python interpreter that are known to have significant\n",
        "# jitter.\n",
        "delta = stats_v1.delta(stats_v0).denoise()\n",
        "\n",
        "# `.transform` is a convenience API for transforming function names. It is\n",
        "# useful for increasing cancelation when ``diff-ing`` instructions, as well as\n",
        "# just generally improving readability.\n",
        "replacements = (\n",
        "    (\"???:void pybind11\", \"pybind11\"),\n",
        "    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n",
        "    (\"at::Tensor, at::Tensor\", \"...\"),\n",
        "    (\"at::Tensor const&, at::Tensor const&\", \"...\"),\n",
        "    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n",
        ")\n",
        "for before, after in replacements:\n",
        "    delta = delta.transform(lambda l: l.replace(before, after))\n",
        "\n",
        "# We can use print options to control how much of the function to display.\n",
        "torch.set_printoptions(linewidth=160)\n",
        "\n",
        "# Once parsed, the instruction counts make clear that passing `a` and `b`\n",
        "# by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping\n",
        "# for the intermediate Tensors, and is also works better with ``pybind11``. This\n",
        "# is consistent with our noisy wall time observations.\n",
        "print(delta)"
      ],
      "metadata": {
        "id": "VpdWT45ThPTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameterizations\n",
        "\n",
        "- Regularization techniques that use a function to add extra structure to parameters to enhance learning and reduce overfitting\n",
        "- Examples include making matricies orthogonal or dividing by a norm\n",
        "\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/parametrizations.html"
      ],
      "metadata": {
        "id": "Rji8DcKhGa_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameterization by hand - Symmetric weights"
      ],
      "metadata": {
        "id": "Ict-YWb9lfeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "def symmetric(X):\n",
        "    return X.triu() + X.triu(1).transpose(-1, -2)\n",
        "\n",
        "X = torch.rand(3, 3)\n",
        "A = symmetric(X)\n",
        "assert torch.allclose(A, A.T)  # A is symmetric\n",
        "print(A)                       # Quick visual check"
      ],
      "metadata": {
        "id": "fuN6M5jRG_IN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31d3f39-df69-4adc-a63c-9bb8c3e21718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5632, 0.2787, 0.9728],\n",
            "        [0.2787, 0.7709, 0.4434],\n",
            "        [0.9728, 0.4434, 0.2891]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSymmetric(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        A = symmetric(self.weight)\n",
        "        return x @ A\n",
        "\n",
        "\n",
        "layer = LinearSymmetric(3)\n",
        "out = layer(torch.rand(8, 3))"
      ],
      "metadata": {
        "id": "6usm46g9l0um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issues:\n",
        "- The layer needs to implemented explicitly\n",
        "- Parameterization is specific to the layer\n",
        "- Recomputes A each pass"
      ],
      "metadata": {
        "id": "0HH4JSYDmQum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch Parameterizations\n"
      ],
      "metadata": {
        "id": "vBJrb5SpmtxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Symmetric(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return X.triu() + X.triu(1).transpose(-1, -2)\n",
        "\n",
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Symmetric())\n",
        "\n",
        "A = layer.weight\n",
        "assert torch.allclose(A, A.T)  # A is symmetric\n",
        "print(A)                       # Quick visual check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09gCIFZm3B2",
        "outputId": "484879c6-1c90-4076-a312-aa231c7118b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2021, -0.0673,  0.5310],\n",
            "        [-0.0673, -0.3446, -0.5545],\n",
            "        [ 0.5310, -0.5545,  0.2324]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Skew(nn.Module):\n",
        "    def forward(self, X):\n",
        "        A = X.triu(1)\n",
        "        return A - A.transpose(-1, -2)\n",
        "\n",
        "\n",
        "cnn = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3)\n",
        "parametrize.register_parametrization(cnn, \"weight\", Skew())\n",
        "# Print a few kernels\n",
        "print(cnn.weight[0, 1])\n",
        "print(cnn.weight[2, 2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjO_hHjfnOX4",
        "outputId": "01fe0d92-835b-4160-eff7-f87b6a92c42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000, -0.0331, -0.0967],\n",
            "        [ 0.0331,  0.0000,  0.0542],\n",
            "        [ 0.0967, -0.0542,  0.0000]], grad_fn=<SelectBackward0>)\n",
            "tensor([[ 0.0000,  0.1168,  0.0632],\n",
            "        [-0.1168,  0.0000,  0.0190],\n",
            "        [-0.0632, -0.0190,  0.0000]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching parametrization\n",
        "class NoisyParametrization(nn.Module):\n",
        "    def forward(self, X):\n",
        "        print(\"Computing the Parametrization\")\n",
        "        return X\n",
        "\n",
        "layer = nn.Linear(4, 4)\n",
        "parametrize.register_parametrization(layer, \"weight\", NoisyParametrization())\n",
        "print(\"Here, layer.weight is recomputed every time we call it\")\n",
        "foo = layer.weight + layer.weight.T\n",
        "bar = layer.weight.sum()\n",
        "with parametrize.cached():\n",
        "    print(\"Here, it is computed just the first time layer.weight is called\")\n",
        "    foo = layer.weight + layer.weight.T\n",
        "    bar = layer.weight.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FepyhuOLonln",
        "outputId": "e3423a75-0a2d-4b4b-80a4-496843e11ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the Parametrization\n",
            "Here, layer.weight is recomputed every time we call it\n",
            "Computing the Parametrization\n",
            "Computing the Parametrization\n",
            "Computing the Parametrization\n",
            "Here, it is computed just the first time layer.weight is called\n",
            "Computing the Parametrization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating Parametrizations\n",
        "class CayleyMap(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"Id\", torch.eye(n))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # (I + X)(I - X)^{-1}\n",
        "        return torch.linalg.solve(self.Id - X, self.Id + X)\n",
        "\n",
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "parametrize.register_parametrization(layer, \"weight\", CayleyMap(3))\n",
        "X = layer.weight\n",
        "print(torch.dist(X.T @ X, torch.eye(3)))  # X is orthogonal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZFomHRqo-Km",
        "outputId": "ac765384-d5e0-40c3-a765-3154f260fb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.7096e-07, grad_fn=<DistBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Parametrizations - right_inverse\n",
        "class Skew(nn.Module):\n",
        "    def forward(self, X):\n",
        "        A = X.triu(1)\n",
        "        return A - A.transpose(-1, -2)\n",
        "\n",
        "    def right_inverse(self, A):\n",
        "        # We assume that A is skew-symmetric\n",
        "        # We take the upper-triangular elements, as these are those used in the forward\n",
        "        return A.triu(1)\n",
        "\n",
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "X = torch.rand(3, 3)\n",
        "X = X - X.T                             # X is now skew-symmetric\n",
        "layer.weight = X                        # Initialize layer.weight to be X\n",
        "print(torch.dist(layer.weight, X))      # layer.weight == X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HhdURCLqPF7",
        "outputId": "197808d4-ef8f-4a6d-ade3-70cb5fc099ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., grad_fn=<DistBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Parametrizations\n",
        "layer = nn.Linear(3, 3)\n",
        "print(\"Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "print(\"\\nParametrized:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.remove_parametrizations(layer, \"weight\")\n",
        "print(\"\\nAfter. Weight has skew-symmetric values but it is unconstrained:\")\n",
        "print(layer)\n",
        "print(layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP7hfB8GqlMb",
        "outputId": "9cd50c74-d471-4620-eeb2-5977feb2808a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0450,  0.3274,  0.4546],\n",
            "        [ 0.1088, -0.1364,  0.3072],\n",
            "        [ 0.4572,  0.4564,  0.3010]], requires_grad=True)\n",
            "\n",
            "Parametrized:\n",
            "ParametrizedLinear(\n",
            "  in_features=3, out_features=3, bias=True\n",
            "  (parametrizations): ModuleDict(\n",
            "    (weight): ParametrizationList(\n",
            "      (0): Skew()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "tensor([[ 0.0000,  0.3274,  0.4546],\n",
            "        [-0.3274,  0.0000,  0.3072],\n",
            "        [-0.4546, -0.3072,  0.0000]], grad_fn=<SubBackward0>)\n",
            "\n",
            "After. Weight has skew-symmetric values but it is unconstrained:\n",
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.3274,  0.4546],\n",
            "        [-0.3274,  0.0000,  0.3072],\n",
            "        [-0.4546, -0.3072,  0.0000]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rollback to original version (non-parametrized)\n",
        "layer = nn.Linear(3, 3)\n",
        "print(\"Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "print(\"\\nParametrized:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.remove_parametrizations(layer, \"weight\", leave_parametrized=False)\n",
        "print(\"\\nAfter. Same as Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBu6if9EqsM2",
        "outputId": "a6397c25-4249-434c-87cc-07234608814c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.3402,  0.1632, -0.1857],\n",
            "        [ 0.5443,  0.0114,  0.3441],\n",
            "        [-0.0584,  0.0381, -0.0388]], requires_grad=True)\n",
            "\n",
            "Parametrized:\n",
            "ParametrizedLinear(\n",
            "  in_features=3, out_features=3, bias=True\n",
            "  (parametrizations): ModuleDict(\n",
            "    (weight): ParametrizationList(\n",
            "      (0): Skew()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "tensor([[ 0.0000,  0.1632, -0.1857],\n",
            "        [-0.1632,  0.0000,  0.3441],\n",
            "        [ 0.1857, -0.3441,  0.0000]], grad_fn=<SubBackward0>)\n",
            "\n",
            "After. Same as Before:\n",
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[ 0.0000,  0.1632, -0.1857],\n",
            "        [ 0.0000,  0.0000,  0.3441],\n",
            "        [ 0.0000,  0.0000,  0.0000]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning\n",
        "\n",
        "- Efficiently compress models by reducing the number of parameters\n",
        "- Assumes base neural networks are over-parametrized\n",
        "\n",
        "\n",
        "- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
      ],
      "metadata": {
        "id": "rgdAcyxHG3AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "gcjnfFMdsFpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet().to(device=device)"
      ],
      "metadata": {
        "id": "MaJ5NkhcsJpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the unpruned module\n",
        "module = model.conv1\n",
        "print(list(module.named_parameters()))\n",
        "print(list(module.named_buffers()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaoGNbX9sMVc",
        "outputId": "473a35dc-4ef8-4d46-9bd4-9a3b38177181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('weight', Parameter containing:\n",
            "tensor([[[[-7.1899e-02,  6.8961e-02,  1.8437e-01,  1.8879e-01, -9.7997e-02],\n",
            "          [ 1.2511e-01, -6.2407e-02,  1.3871e-01, -8.1274e-02,  1.1043e-02],\n",
            "          [-1.8767e-01, -3.7982e-02,  8.5971e-02, -2.9066e-02,  1.0911e-01],\n",
            "          [ 6.7399e-02, -6.9951e-02,  1.4982e-01,  1.5166e-01, -1.9563e-01],\n",
            "          [-6.7001e-02, -7.9019e-02, -9.1224e-02, -1.9324e-01,  3.8285e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1506e-01,  5.4059e-04, -1.7525e-01, -2.9687e-02, -1.0025e-01],\n",
            "          [-1.4979e-01,  1.9993e-01, -1.7546e-01,  1.4791e-01,  1.4370e-02],\n",
            "          [ 1.3886e-01, -1.4606e-01, -4.8855e-02, -9.5823e-02, -1.2789e-01],\n",
            "          [-4.6367e-02, -1.4612e-01, -6.9884e-02,  7.4581e-02, -1.9142e-01],\n",
            "          [ 1.8051e-01, -1.6570e-02, -9.7502e-03,  1.0295e-01,  1.6524e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3684e-01,  6.5224e-02, -1.9694e-01,  3.7557e-02,  2.5860e-02],\n",
            "          [-1.8559e-01,  6.9837e-02,  1.2736e-01,  1.8080e-01, -8.8097e-02],\n",
            "          [ 1.2533e-01, -1.5148e-01, -1.7532e-01, -1.4258e-01,  3.4855e-02],\n",
            "          [ 1.9768e-01,  1.0762e-01,  4.5908e-02,  7.4921e-02, -8.7886e-04],\n",
            "          [-1.0661e-01,  1.6609e-01,  8.3094e-02, -4.2113e-02,  2.6347e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1153e-01, -1.8284e-01,  7.2938e-02,  1.6195e-01, -7.7871e-02],\n",
            "          [ 4.1986e-03,  4.4482e-02,  1.1372e-01, -1.9917e-01, -9.0033e-02],\n",
            "          [ 6.0239e-02, -3.2248e-02,  2.1569e-02, -1.4785e-01,  1.3037e-02],\n",
            "          [-7.1857e-02, -8.6541e-02, -6.6426e-02, -6.3905e-03, -1.6149e-01],\n",
            "          [-7.9736e-02,  4.1636e-02,  4.0316e-02, -1.2811e-02,  1.2540e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 7.3596e-03, -1.3970e-01, -9.3937e-06, -1.7231e-01,  1.9763e-01],\n",
            "          [-9.0138e-02,  1.2474e-01,  1.0340e-01, -9.9173e-02, -1.2581e-01],\n",
            "          [-6.2170e-02, -1.9600e-01, -1.4715e-01, -8.0006e-02,  3.0520e-02],\n",
            "          [ 5.1633e-02,  5.3113e-02,  1.0339e-01, -1.1104e-01, -1.2827e-01],\n",
            "          [-8.0926e-02,  8.6963e-04,  1.6412e-01, -1.7297e-01, -1.9349e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.4767e-02, -5.4910e-02, -1.8952e-01, -3.3994e-02, -1.7457e-01],\n",
            "          [ 1.6803e-01,  3.4428e-02,  1.0131e-01,  2.1989e-02,  1.4900e-01],\n",
            "          [ 1.3632e-01,  4.1565e-02,  2.0015e-02,  3.7757e-02, -1.5006e-01],\n",
            "          [ 1.7527e-01,  8.2628e-03,  1.5528e-02,  1.7336e-01, -2.9162e-02],\n",
            "          [ 5.7187e-02,  2.9948e-02,  7.5305e-02, -1.2515e-02, -1.2821e-01]]]],\n",
            "       requires_grad=True)), ('bias', Parameter containing:\n",
            "tensor([ 0.1289, -0.0618,  0.0900, -0.1107, -0.0214, -0.0261],\n",
            "       requires_grad=True))]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random pruning\n",
        "prune.random_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# Can also prune bias\n",
        "# prune.l1_unstructured(module, name=\"bias\", amount=3)\n",
        "\n",
        "print(list(module.named_parameters()))\n",
        "print(list(module.named_buffers()))\n",
        "print(module.weight)\n",
        "print(module._forward_pre_hooks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSijyasgsSIF",
        "outputId": "6e7e927f-159f-44d2-a12e-25ddd6f6f970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.1289, -0.0618,  0.0900, -0.1107, -0.0214, -0.0261],\n",
            "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[[[-7.1899e-02,  6.8961e-02,  1.8437e-01,  1.8879e-01, -9.7997e-02],\n",
            "          [ 1.2511e-01, -6.2407e-02,  1.3871e-01, -8.1274e-02,  1.1043e-02],\n",
            "          [-1.8767e-01, -3.7982e-02,  8.5971e-02, -2.9066e-02,  1.0911e-01],\n",
            "          [ 6.7399e-02, -6.9951e-02,  1.4982e-01,  1.5166e-01, -1.9563e-01],\n",
            "          [-6.7001e-02, -7.9019e-02, -9.1224e-02, -1.9324e-01,  3.8285e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1506e-01,  5.4059e-04, -1.7525e-01, -2.9687e-02, -1.0025e-01],\n",
            "          [-1.4979e-01,  1.9993e-01, -1.7546e-01,  1.4791e-01,  1.4370e-02],\n",
            "          [ 1.3886e-01, -1.4606e-01, -4.8855e-02, -9.5823e-02, -1.2789e-01],\n",
            "          [-4.6367e-02, -1.4612e-01, -6.9884e-02,  7.4581e-02, -1.9142e-01],\n",
            "          [ 1.8051e-01, -1.6570e-02, -9.7502e-03,  1.0295e-01,  1.6524e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3684e-01,  6.5224e-02, -1.9694e-01,  3.7557e-02,  2.5860e-02],\n",
            "          [-1.8559e-01,  6.9837e-02,  1.2736e-01,  1.8080e-01, -8.8097e-02],\n",
            "          [ 1.2533e-01, -1.5148e-01, -1.7532e-01, -1.4258e-01,  3.4855e-02],\n",
            "          [ 1.9768e-01,  1.0762e-01,  4.5908e-02,  7.4921e-02, -8.7886e-04],\n",
            "          [-1.0661e-01,  1.6609e-01,  8.3094e-02, -4.2113e-02,  2.6347e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1153e-01, -1.8284e-01,  7.2938e-02,  1.6195e-01, -7.7871e-02],\n",
            "          [ 4.1986e-03,  4.4482e-02,  1.1372e-01, -1.9917e-01, -9.0033e-02],\n",
            "          [ 6.0239e-02, -3.2248e-02,  2.1569e-02, -1.4785e-01,  1.3037e-02],\n",
            "          [-7.1857e-02, -8.6541e-02, -6.6426e-02, -6.3905e-03, -1.6149e-01],\n",
            "          [-7.9736e-02,  4.1636e-02,  4.0316e-02, -1.2811e-02,  1.2540e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 7.3596e-03, -1.3970e-01, -9.3937e-06, -1.7231e-01,  1.9763e-01],\n",
            "          [-9.0138e-02,  1.2474e-01,  1.0340e-01, -9.9173e-02, -1.2581e-01],\n",
            "          [-6.2170e-02, -1.9600e-01, -1.4715e-01, -8.0006e-02,  3.0520e-02],\n",
            "          [ 5.1633e-02,  5.3113e-02,  1.0339e-01, -1.1104e-01, -1.2827e-01],\n",
            "          [-8.0926e-02,  8.6963e-04,  1.6412e-01, -1.7297e-01, -1.9349e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.4767e-02, -5.4910e-02, -1.8952e-01, -3.3994e-02, -1.7457e-01],\n",
            "          [ 1.6803e-01,  3.4428e-02,  1.0131e-01,  2.1989e-02,  1.4900e-01],\n",
            "          [ 1.3632e-01,  4.1565e-02,  2.0015e-02,  3.7757e-02, -1.5006e-01],\n",
            "          [ 1.7527e-01,  8.2628e-03,  1.5528e-02,  1.7336e-01, -2.9162e-02],\n",
            "          [ 5.7187e-02,  2.9948e-02,  7.5305e-02, -1.2515e-02, -1.2821e-01]]]],\n",
            "       requires_grad=True))]\n",
            "[('weight_mask', tensor([[[[0., 0., 0., 1., 1.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [1., 0., 0., 1., 0.],\n",
            "          [0., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 1., 0., 1., 0.],\n",
            "          [0., 1., 0., 0., 0.],\n",
            "          [0., 1., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 1., 1.],\n",
            "          [1., 0., 0., 0., 0.],\n",
            "          [0., 1., 0., 0., 1.],\n",
            "          [1., 0., 0., 0., 0.],\n",
            "          [0., 0., 1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 1., 0.],\n",
            "          [0., 1., 0., 0., 0.],\n",
            "          [0., 1., 1., 0., 1.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 1., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 1., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 1., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [1., 0., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0., 0., 1.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.],\n",
            "          [0., 0., 0., 0., 0.]]]]))]\n",
            "tensor([[[[-0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8879e-01, -9.7997e-02],\n",
            "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
            "          [ 6.7399e-02, -0.0000e+00,  0.0000e+00,  1.5166e-01, -0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00,  5.4059e-04, -0.0000e+00, -2.9687e-02, -0.0000e+00],\n",
            "          [-0.0000e+00,  1.9993e-01, -0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "          [ 0.0000e+00, -1.4606e-01, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
            "          [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  3.7557e-02,  2.5860e-02],\n",
            "          [-1.8559e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
            "          [ 0.0000e+00, -1.5148e-01, -0.0000e+00, -0.0000e+00,  3.4855e-02],\n",
            "          [ 1.9768e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
            "          [-0.0000e+00,  0.0000e+00,  8.3094e-02, -0.0000e+00,  0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00, -0.0000e+00,  0.0000e+00,  1.6195e-01, -0.0000e+00],\n",
            "          [ 0.0000e+00,  4.4482e-02,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "          [ 0.0000e+00, -3.2248e-02,  2.1569e-02, -0.0000e+00,  1.3037e-02],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "          [-0.0000e+00,  0.0000e+00,  4.0316e-02, -0.0000e+00,  0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000e+00, -0.0000e+00, -9.3937e-06, -0.0000e+00,  0.0000e+00],\n",
            "          [-0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "          [-0.0000e+00, -0.0000e+00, -0.0000e+00, -8.0006e-02,  0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
            "          [-8.0926e-02,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00, -1.7457e-01],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00],\n",
            "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00]]]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "OrderedDict([(4, <torch.nn.utils.prune.PruningContainer object at 0x7a7c6fe70130>)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stacked Pruning - Prune the #of channels\n",
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
        "\n",
        "# As we can verify, this will zero out all the connections corresponding to\n",
        "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
        "# previous mask.\n",
        "print(module.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Lvi8rttEq3",
        "outputId": "060e7ee8-ff25-4685-830e-07f2797838ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.0000,  0.0000,  0.0000,  0.1888, -0.0980],\n",
            "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0674, -0.0000,  0.0000,  0.1517, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0005, -0.0000, -0.0297, -0.0000],\n",
            "          [-0.0000,  0.1999, -0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.1461, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000, -0.0000,  0.0376,  0.0259],\n",
            "          [-0.1856,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.1515, -0.0000, -0.0000,  0.0349],\n",
            "          [ 0.1977,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0831, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serializing pruned model\n",
        "print(model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMUC4qljui4V",
        "outputId": "9fc72e71-16ef-4346-fcd7-8505067c3d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['conv1.bias', 'conv1.weight_orig', 'conv1.weight_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make pruned state permanent\n",
        "prune.remove(module, 'weight')\n",
        "print(list(module.named_parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJLESsAPu_j6",
        "outputId": "fbbf8d51-af6f-42e0-efb7-26e95143b7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('bias', Parameter containing:\n",
            "tensor([ 0.1289, -0.0618,  0.0900, -0.1107, -0.0214, -0.0261],\n",
            "       requires_grad=True)), ('weight', Parameter containing:\n",
            "tensor([[[[-0.0000,  0.0000,  0.0000,  0.1888, -0.0980],\n",
            "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0674, -0.0000,  0.0000,  0.1517, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0005, -0.0000, -0.0297, -0.0000],\n",
            "          [-0.0000,  0.1999, -0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000, -0.1461, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000,  0.0000, -0.0000,  0.0376,  0.0259],\n",
            "          [-0.1856,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000, -0.1515, -0.0000, -0.0000,  0.0349],\n",
            "          [ 0.1977,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0831, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000],\n",
            "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]],\n",
            "\n",
            "\n",
            "        [[[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000],\n",
            "          [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000]]]], requires_grad=True))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruning multiple parameters\n",
        "new_model = LeNet()\n",
        "for name, module in new_model.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0e71tDvgAB",
        "outputId": "3752e1bc-1dd3-432d-9477-195b7b57813d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global pruning\n",
        "model = LeNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc3.weight == 0))\n",
        "        / float(model.fc3.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb1N_FwLwJUW",
        "outputId": "530b65af-f592-409d-e954-fb0fe4ed57ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity in conv1.weight: 6.00%\n",
            "Sparsity in conv2.weight: 14.25%\n",
            "Sparsity in fc1.weight: 22.18%\n",
            "Sparsity in fc2.weight: 11.97%\n",
            "Sparsity in fc3.weight: 10.95%\n",
            "Global sparsity: 20.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom pruning\n",
        "class FooBarPruningMethod(prune.BasePruningMethod):\n",
        "    \"\"\"Prune every other entry in a tensor\n",
        "    \"\"\"\n",
        "    PRUNING_TYPE = 'unstructured'\n",
        "\n",
        "    def compute_mask(self, t, default_mask):\n",
        "        mask = default_mask.clone()\n",
        "        mask.view(-1)[::2] = 0\n",
        "        return mask\n",
        "\n",
        "def foobar_unstructured(module, name):\n",
        "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
        "    by removing every other entry in the tensors.\n",
        "    Modifies module in place (and also return the modified module)\n",
        "    by:\n",
        "    1) adding a named buffer called `name+'_mask'` corresponding to the\n",
        "    binary mask applied to the parameter `name` by the pruning method.\n",
        "    The parameter `name` is replaced by its pruned version, while the\n",
        "    original (unpruned) parameter is stored in a new parameter named\n",
        "    `name+'_orig'`.\n",
        "\n",
        "    Args:\n",
        "        module (nn.Module): module containing the tensor to prune\n",
        "        name (string): parameter name within `module` on which pruning\n",
        "                will act.\n",
        "\n",
        "    Returns:\n",
        "        module (nn.Module): modified (i.e. pruned) version of the input\n",
        "            module\n",
        "\n",
        "    Examples:\n",
        "        >>> m = nn.Linear(3, 4)\n",
        "        >>> foobar_unstructured(m, name='bias')\n",
        "    \"\"\"\n",
        "    FooBarPruningMethod.apply(module, name)\n",
        "    return module\n",
        "\n",
        "model = LeNet()\n",
        "foobar_unstructured(model.fc3, name='bias')\n",
        "\n",
        "print(model.fc3.bias_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wotEN1iwhii",
        "outputId": "446605f9-0607-4544-f227-9481c4fc74d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbD2qP-BxBnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}