# -*- coding: utf-8 -*-
"""model_optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWuQENMGVX7UUKvjYoE9fCQz_uqoyIfh

# Model Optimization with PyTorch

- See: https://pytorch.org/tutorials/index.html

## Hyperparameter Tuning with Ray Tune

- From: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html
- Ray Tune Docs: https://docs.ray.io/en/latest/tune/index.html

Ray Tune is an open source library for hyperparameter tuning. It contains the latest search algorithms, integrates with TensorBoard and other analysis libraries, and supports distributed training.

### CIFAR10 Classifier
"""

# Commented out IPython magic to ensure Python compatibility.
# HPT
# %pip install ray[tune]
# %pip install ray

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np


# HPT
from functools import partial
import os
import tempfile
from pathlib import Path
from torch.utils.data import random_split
from ray import tune
from ray import train
from ray.train import Checkpoint, get_checkpoint
from ray.tune.schedulers import ASHAScheduler
import ray.cloudpickle as pickle

# HPT
# Wrap the data loaders and pass a directory to access data from different trials

def load_data(data_dir="./data"):
    transform = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] # Normalize from [0, 1] to [-1, 1]
    )

    trainset = torchvision.datasets.CIFAR10(
        root=data_dir, train=True, download=True, transform=transform
    )

    testset = torchvision.datasets.CIFAR10(
        root=data_dir, train=False, download=True, transform=transform
    )

    return trainset, testset

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def imshow(img):
    img = img / 2 + 0.3 # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# HPT
# Make parameters of the model (l1, l2) configurable to allow for tuning

class Net(nn.Module):
    def __init__(self, l1=120, l2=84):
        super().__init__()
        # input is 3 x 32 x 32
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, l1)
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, len(classes))

    def forward(self, x):
        # 3 x 32 x 32
        x = self.pool(F.relu(self.conv1(x)))
        # 6 x 14 x 14
        x = self.pool(F.relu(self.conv2(x)))
        # 16 x 5 x 5
        x = torch.flatten(x, 1) # Flatten dimensions except for batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# HPT
# Wrap the training function with a few
#  - config specifies the hyperparameters to train with (l1, l2, lr, bs)
#  - data_dir gives the dirctory to load and store data
#  - Use the GPU and DataParallel if available
#  - Saving training checkpoints for advanced schedulers and fault tolerance
#  - Sending validation loss and accuracy to Ray Tune for evaluation

def train_cifar(config, data_dir=None):
    net = Net(config["l1"], config["l2"])

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() > 1:
            net = nn.DataParallel(net)
    net.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)


    checkpoint = get_checkpoint()
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "rb") as fp:
                checkpoint_state = pickle.laod(fp)
            start_epoch = checkpoint_state["epoch"]
            net.load_state_dict(checkpoint_state["net_state_dict"])
            optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
    else:
        start_epoch = 0


    trainset, testset = load_data(data_dir)

    test_abs = int(len(trainset) * 0.8)
    train_subset, val_subset = random_split(
        trainset, [test_abs, len(trainset) - test_abs]
    )

    trainloader = torch.utils.data.DataLoader(
        train_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
    )

    valloader = torch.utils.data.DataLoader(
        val_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
    )


    for epoch in range(start_epoch, 10):
        running_loss = 0.0
        epoch_steps = 0
        for i, (inputs, labels) in enumerate(trainloader, 0):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            epoch_steps += 1
            if i % 2000 == 1999:
                print(f"[{epoch+1}, {i + 1:5d}] loss: {running_loss / epoch_steps:.3f}")
                running_loss = 0.0
                epoch_steps = 0


        val_loss = 0.0
        val_steps = 0
        total = 0
        correct = 0
        for i, (inputs, labels) in enumerate(valloader, 0):
            with torch.no_grad():
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = net(inputs)

                _, predicted = torch.max(outputs.detach(), 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                loss = criterion(outputs, labels)
                val_loss += loss.item()
                val_steps += 1


        checkpoint_data = {
            "epoch" : epoch,
            "net_state_dict": net.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        }
        with tempfile.TemporaryDirectory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "wb") as fp:
                pickle.dump(checkpoint_data, fp)

            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            train.report(
                {"loss": val_loss / val_steps, "accuracy": correct / total},
                checkpoint=checkpoint,
            )

    print("Finished Training")

# HPT
# Wrap a test accuracy function

def test_accuracy(net, device="cpu"):
    batch_size = 4
    trainset, testset = load_data()
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    correct = 0
    total = 0
    correct_pred = {classname: 0 for classname in classes}
    total_pred = {classname: 0 for classname in classes}
    with torch.no_grad():
        for (images, labels) in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.detach(), 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            for label, prediction in zip(labels, predicted):
                total_pred[classes[label]] += 1
                if label == prediction:
                    correct_pred[classes[label]] += 1

        dataiter = iter(testloader)
        images, labels = next(dataiter)

        imshow(torchvision.utils.make_grid(images))
        print('GroundTruth: ', ' '.join(f"{classes[labels[j]]:5s}" for j in range(batch_size)))

        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)
        print('Predicted: ', ' '.join(f"{classes[predicted[j]]:5s}" for j in range(batch_size)))


    return correct / total, correct_pred, total_pred

# HPT
# Wrap the tune functionality
# - Setup the config parameters
# - Use the ASHAScheduler, which terminates bad performing trials early
# - Specify the available CPU and GPU (can be a fraction)

def tune_model(num_samples=10, max_num_epochs=10, gpus_per_trial=1):
    data_dir = os.path.abspath("./data")
    load_data(data_dir)

    config = {
        "l1": tune.choice([2 ** i for i in range(4, 9)]),
        "l2": tune.choice([2 ** i for i in range(4, 9)]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "batch_size": tune.choice([2 ** i for i in range(1, 5)])
    }

    scheduler = ASHAScheduler(
        metric="loss",
        mode="min",
        max_t=max_num_epochs,
        grace_period=1,
        reduction_factor=2,
    )

    result = tune.run(
        partial(train_cifar, data_dir=data_dir),
        resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
    )

    return result

# Check devices
device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"
    print(f"cuda device count: {torch.cuda.device_count()}")
print(f"Device: {device}")
gpus_per_trial = 1

# HPT
# Run the optimization
result = tune_model(num_samples=2, max_num_epochs=5, gpus_per_trial=gpus_per_trial)

# Show the results of the best model

best_trial = result.get_best_trial("loss", "min", "last")
print(f"Best trial config: {best_trial.config}")
print(f"Best trial final validation loss: {best_trial.last_result['loss']}")
print(f"Best trial final validation accuracy: {best_trial.last_result['accuracy']}")

best_trained_model = Net(best_trial.config["l1"], best_trial.config["l2"])
device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"
    if gpus_per_trial > 1:
        best_trained_model = nn.DataParallel(best_trained_model)
best_trained_model.to(device)

best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric="accuracy", mode="max")
with best_checkpoint.as_directory() as checkpoint_dir:
    data_path = Path(checkpoint_dir) / "data.pkl"
    print("Checkpoint Path: ", data_path)
    with open(data_path, "rb") as fp:
        best_checkpoint_data = pickle.load(fp)

    best_trained_model.load_state_dict(best_checkpoint_data["net_state_dict"])
    test_acc, correct_pred, total_pred = test_accuracy(best_trained_model, device)
    print("Best trial test set accuracy: {}".format(test_acc))

    for classname, correct in correct_pred.items():
        accuracy = correct / total_pred[classname] * 100
        print(f'Accuracy of {classname:5s} is {accuracy:.1f} %')

# Commented out IPython magic to ensure Python compatibility.
# Remove the artifacts
# /tmp/ray/*
# /root/ray_results/*
# %rm -r /tmp/ray/*
# %rm -r /root/ray_results/*

