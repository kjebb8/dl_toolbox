# -*- coding: utf-8 -*-
"""model_optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWuQENMGVX7UUKvjYoE9fCQz_uqoyIfh

# Model Optimization with PyTorch

- See: https://pytorch.org/tutorials/index.html

## Hyperparameter Tuning with Ray Tune

- From: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html
- Ray Tune Docs: https://docs.ray.io/en/latest/tune/index.html
- Note: Tutorial is based on an original CNN tutorial. The comments marked by "HPT" show the new changes/additions with the hyperparameter tuning tutorial

Ray Tune is an open source library for hyperparameter tuning. It contains the latest search algorithms, integrates with TensorBoard and other analysis libraries, and supports distributed training.

### CIFAR10 Classifier
"""

# Commented out IPython magic to ensure Python compatibility.
# HPT
# %pip install ray[tune]
# %pip install ray

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np


# HPT
from functools import partial
import os
import tempfile
from pathlib import Path
from torch.utils.data import random_split
from ray import tune
from ray import train
from ray.train import Checkpoint, get_checkpoint
from ray.tune.schedulers import ASHAScheduler
import ray.cloudpickle as pickle

# HPT
# Wrap the data loaders and pass a directory to access data from different trials

def load_data(data_dir="./data"):
    transform = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] # Normalize from [0, 1] to [-1, 1]
    )

    trainset = torchvision.datasets.CIFAR10(
        root=data_dir, train=True, download=True, transform=transform
    )

    testset = torchvision.datasets.CIFAR10(
        root=data_dir, train=False, download=True, transform=transform
    )

    return trainset, testset

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def imshow(img):
    img = img / 2 + 0.3 # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

# HPT
# Make parameters of the model (l1, l2) configurable to allow for tuning

class Net(nn.Module):
    def __init__(self, l1=120, l2=84):
        super().__init__()
        # input is 3 x 32 x 32
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, l1)
        self.fc2 = nn.Linear(l1, l2)
        self.fc3 = nn.Linear(l2, len(classes))

    def forward(self, x):
        # 3 x 32 x 32
        x = self.pool(F.relu(self.conv1(x)))
        # 6 x 14 x 14
        x = self.pool(F.relu(self.conv2(x)))
        # 16 x 5 x 5
        x = torch.flatten(x, 1) # Flatten dimensions except for batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# HPT
# Wrap the training function with a few
#  - config specifies the hyperparameters to train with (l1, l2, lr, bs)
#  - data_dir gives the dirctory to load and store data
#  - Use the GPU and DataParallel if available
#  - Saving training checkpoints for advanced schedulers and fault tolerance
#  - Sending validation loss and accuracy to Ray Tune for evaluation

def train_cifar(config, data_dir=None):
    net = Net(config["l1"], config["l2"])

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda:0"
        if torch.cuda.device_count() > 1:
            net = nn.DataParallel(net)
    net.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=config["lr"], momentum=0.9)


    checkpoint = get_checkpoint()
    if checkpoint:
        with checkpoint.as_directory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "rb") as fp:
                checkpoint_state = pickle.laod(fp)
            start_epoch = checkpoint_state["epoch"]
            net.load_state_dict(checkpoint_state["net_state_dict"])
            optimizer.load_state_dict(checkpoint_state["optimizer_state_dict"])
    else:
        start_epoch = 0


    trainset, testset = load_data(data_dir)

    test_abs = int(len(trainset) * 0.8)
    train_subset, val_subset = random_split(
        trainset, [test_abs, len(trainset) - test_abs]
    )

    trainloader = torch.utils.data.DataLoader(
        train_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
    )

    valloader = torch.utils.data.DataLoader(
        val_subset, batch_size=int(config["batch_size"]), shuffle=True, num_workers=8
    )


    for epoch in range(start_epoch, 10):
        running_loss = 0.0
        epoch_steps = 0
        for i, (inputs, labels) in enumerate(trainloader, 0):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            epoch_steps += 1
            if i % 2000 == 1999:
                print(f"[{epoch+1}, {i + 1:5d}] loss: {running_loss / epoch_steps:.3f}")
                running_loss = 0.0
                epoch_steps = 0


        val_loss = 0.0
        val_steps = 0
        total = 0
        correct = 0
        for i, (inputs, labels) in enumerate(valloader, 0):
            with torch.no_grad():
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = net(inputs)

                _, predicted = torch.max(outputs.detach(), 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                loss = criterion(outputs, labels)
                val_loss += loss.item()
                val_steps += 1


        checkpoint_data = {
            "epoch" : epoch,
            "net_state_dict": net.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        }
        with tempfile.TemporaryDirectory() as checkpoint_dir:
            data_path = Path(checkpoint_dir) / "data.pkl"
            with open(data_path, "wb") as fp:
                pickle.dump(checkpoint_data, fp)

            checkpoint = Checkpoint.from_directory(checkpoint_dir)
            train.report(
                {"loss": val_loss / val_steps, "accuracy": correct / total},
                checkpoint=checkpoint,
            )

    print("Finished Training")

# HPT
# Wrap a test accuracy function

def test_accuracy(net, device="cpu"):
    batch_size = 4
    trainset, testset = load_data()
    testloader = torch.utils.data.DataLoader(
        testset, batch_size=batch_size, shuffle=False, num_workers=2
    )

    correct = 0
    total = 0
    correct_pred = {classname: 0 for classname in classes}
    total_pred = {classname: 0 for classname in classes}
    with torch.no_grad():
        for (images, labels) in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.detach(), 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            for label, prediction in zip(labels, predicted):
                total_pred[classes[label]] += 1
                if label == prediction:
                    correct_pred[classes[label]] += 1

        dataiter = iter(testloader)
        images, labels = next(dataiter)

        imshow(torchvision.utils.make_grid(images))
        print('GroundTruth: ', ' '.join(f"{classes[labels[j]]:5s}" for j in range(batch_size)))

        outputs = net(images.to(device))
        _, predicted = torch.max(outputs, 1)
        print('Predicted: ', ' '.join(f"{classes[predicted[j]]:5s}" for j in range(batch_size)))


    return correct / total, correct_pred, total_pred

# HPT
# Wrap the tune functionality
# - Setup the config parameters
# - Use the ASHAScheduler, which terminates bad performing trials early
# - Specify the available CPU and GPU (can be a fraction)

def tune_model(num_samples=10, max_num_epochs=10, gpus_per_trial=1):
    data_dir = os.path.abspath("./data")
    load_data(data_dir)

    config = {
        "l1": tune.choice([2 ** i for i in range(4, 9)]),
        "l2": tune.choice([2 ** i for i in range(4, 9)]),
        "lr": tune.loguniform(1e-4, 1e-1),
        "batch_size": tune.choice([2 ** i for i in range(1, 5)])
    }

    scheduler = ASHAScheduler(
        metric="loss",
        mode="min",
        max_t=max_num_epochs,
        grace_period=1,
        reduction_factor=2,
    )

    result = tune.run(
        partial(train_cifar, data_dir=data_dir),
        resources_per_trial={"cpu": 2, "gpu": gpus_per_trial},
        config=config,
        num_samples=num_samples,
        scheduler=scheduler,
    )

    return result

# Check devices
device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"
    print(f"cuda device count: {torch.cuda.device_count()}")
print(f"Device: {device}")
gpus_per_trial = 1

# HPT
# Run the optimization
result = tune_model(num_samples=2, max_num_epochs=5, gpus_per_trial=gpus_per_trial)

# Show the results of the best model

best_trial = result.get_best_trial("loss", "min", "last")
print(f"Best trial config: {best_trial.config}")
print(f"Best trial final validation loss: {best_trial.last_result['loss']}")
print(f"Best trial final validation accuracy: {best_trial.last_result['accuracy']}")

best_trained_model = Net(best_trial.config["l1"], best_trial.config["l2"])
device = "cpu"
if torch.cuda.is_available():
    device = "cuda:0"
    if gpus_per_trial > 1:
        best_trained_model = nn.DataParallel(best_trained_model)
best_trained_model.to(device)

best_checkpoint = result.get_best_checkpoint(trial=best_trial, metric="accuracy", mode="max")
with best_checkpoint.as_directory() as checkpoint_dir:
    data_path = Path(checkpoint_dir) / "data.pkl"
    print("Checkpoint Path: ", data_path)
    with open(data_path, "rb") as fp:
        best_checkpoint_data = pickle.load(fp)

    best_trained_model.load_state_dict(best_checkpoint_data["net_state_dict"])
    test_acc, correct_pred, total_pred = test_accuracy(best_trained_model, device)
    print("Best trial test set accuracy: {}".format(test_acc))

    for classname, correct in correct_pred.items():
        accuracy = correct / total_pred[classname] * 100
        print(f'Accuracy of {classname:5s} is {accuracy:.1f} %')

# Commented out IPython magic to ensure Python compatibility.
# Remove the artifacts
# /tmp/ray/*
# /root/ray_results/*
# %rm -r /tmp/ray/*
# %rm -r /root/ray_results/*

"""## Performance Profiling

### PyTorch Profiler

- PyTorch recipe: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html

- Profiler supports multithreaded models
- Profiler runs in the same thread as the operation but it will also profile child operators that might run in another thread
- Concurrently-running profilers will be scoped to their own thread to prevent mixing of results
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install torch torchvision

import torch
import torchvision.models as models
from torch.profiler import profile, record_function, ProfilerActivity

model = models.resnet18()
inputs = torch.randn(5,3,224,224)

"""---
#### Profile CPU Execution Time
---
"""

with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
    with record_function("model_inference"):
        model(inputs)

print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

print(prof.key_averages().table(sort_by="self_cpu_time_total", row_limit=10)) # "Self" Excludes time in child operations

print(prof.key_averages(group_by_input_shape=True).table(sort_by="cpu_time_total", row_limit=10)) # Finer granularity

"""
---
#### Profile GPU Execution Time
---
"""

model = models.resnet18().cuda()
inputs = torch.randn(5,3,224,224).cuda()
with profile(activities=[
    ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    with record_function("model_inference"):
        model(inputs)

print(prof.key_averages().table(sort_by="self_cuda_time_total", row_limit=10))

"""
---
#### Profile Memory
---"""

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof:
    with record_function("model_inference"):
        model(inputs)

print(prof.key_averages().table(sort_by="self_cpu_memory_usage", row_limit=10))

print(prof.key_averages().table(sort_by="self_cuda_memory_usage", row_limit=10))

"""
---
#### Tracing
---"""

model = models.resnet18().cuda()
inputs = torch.randn(5, 3, 224, 224).cuda()

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    model(inputs)

prof.export_chrome_trace("trace.json")

# Open trace file in Chrome window at chrome://tracing

"""
---
#### Stack Traces
---"""

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    with_stack=True,
) as prof:
    with record_function("model_inference"):
        model(inputs)

print(prof.key_averages(group_by_stack_n=5).table(sort_by="self_cuda_time_total", row_limit=2))
# Not working, may be a PyTorch issue

"""---
#### Long-running Jobs
---

- Jobs like training can take a long time and have different requirements
- Tracing all of a long job can be very slow and output large trace files

schedule - specifies a function that takes an integer argument (step number) as an input and returns an action for the profiler, the best way to use this parameter is to use torch.profiler.schedule helper function that can generate a schedule for you;

on_trace_ready - specifies a function that takes a reference to the profiler as an input and is called by the profiler each time the new trace is ready.
"""

from torch.profiler import schedule

my_schedule = schedule(
    skip_first=10, # ignore first 10 steps
    wait=5,        # Idle time before collection
    warmup=1,      # Initial discarded traces due to extra overhead
    active=3,      # Number of steps to record data
    repeat=2)      # Upper bound on the number of cycles (wait,warmup,active) to collect

def trace_handler(p):
    output = p.key_averages().table(sort_by="self_cuda_time_total", row_limit=10)
    print(output)
    p.export_chrome_trace("/tmp/trace_" + str(p.step_num) + ".json")

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=1,
        warmup=1,
        active=2),
    on_trace_ready=trace_handler
) as p:
    for idx in range(8):
        model(inputs)
        p.step()

"""### Profile Modules

- PyTorch tutorial: https://pytorch.org/tutorials/beginner/profiler.html
"""

import torch
import numpy as np
from torch import nn
import torch.autograd.profiler as profiler

class MyModule(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super(MyModule, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

    def forward(self, input, mask):
        with profiler.record_function("Linear"):
            out = self.linear(input)

        with profiler.record_function("Mask"):
            threshold = out.sum(axis=1).mean().item()
            hi_idx = np.argwhere(mask.cpu().numpy() > threshold) # Copy to CPU
            hi_idx = torch.from_numpy(hi_idx).cuda() # Copy to CUDA

        return out, hi_idx

model = MyModule(500, 10).cuda()
input = torch.rand(128,500).cuda()
mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()

# warm up cuda
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask)

print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

# Improve the memory performace of Mask by casting as float instead of double
mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()

# warm up cuda
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask)

print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

# Improve the time performance by eliminating copies
class MyModule(nn.Module):
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super(MyModule, self).__init__()
        self.linear = nn.Linear(in_features, out_features, bias)

    def forward(self, input, mask):
        with profiler.record_function("Linear"):
            out = self.linear(input)

        with profiler.record_function("Mask"):
            threshold = out.sum(axis=1).mean()
            hi_idx = (mask > threshold).nonzero(as_tuple=True)

        return out, hi_idx


model = MyModule(500, 10).cuda()
input = torch.rand(128, 500).cuda()
mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()

# warm-up
model(input, mask)

with profiler.profile(with_stack=True, profile_memory=True) as prof:
    out, idx = model(input, mask)

print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))

"""### Tensorboard

- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html

Also check Holistic Trace Analysis: https://hta.readthedocs.io/en/latest/index.html

- PyTorch tutorial: https://pytorch.org/tutorials/beginner/hta_intro_tutorial.html
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install HolisticTraceAnalysis
# %pip install torch_tb_profiler

import torch
import torch.nn
import torch.optim
import torch.profiler
import torch.utils.data
import torchvision.datasets
import torchvision.models
import torchvision.transforms as T

transform = T.Compose(
    [T.Resize(224),
     T.ToTensor(),
     T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)

# device = torch.device("cuda:0")
device = torch.device('cpu')
model = torchvision.models.resnet18(weights='IMAGENET1K_V1').to(device)
criterion = torch.nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
model.train()
print("Resnet model")

def train(data):
    inputs, labels = data[0].to(device=device), data[1].to(device=device)
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

with torch.profiler.profile(
    activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
    on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name='./log/resnet18', use_gzip=False), # Generates and saves tensorboard files
    record_shapes=True,
    profile_memory=True,
    with_stack=True
) as prof:
    for step, batch_data in enumerate(train_loader):
        train(batch_data)
        prof.step()  # Need to call this at each step to notify profiler of steps' boundary.
        if step >= 1 + (1 + 3) * 2:
            break

# Tensorboard
# tensorboard --logdir=./log
# http://localhost:6006/#pytorch_profiler

# HTA
from hta.trace_analysis import TraceAnalysis
trace_dir = "/content/log/resnet18/"
trace_files = {0 : '8c230de2449a_145.1717189838062872263.pt.trace.json'}
analyzer = TraceAnalysis(trace_dir=trace_dir, trace_files=trace_files)

# Try HTA when GPU available
# temporal_breakdown_df = analyzer.get_temporal_breakdown()

"""## Benchmark

- PyTorch tutorial: https://pytorch.org/tutorials/recipes/recipes/benchmark.html
"""



"""## Parameterizations

- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/parametrizations.html
"""



"""## Pruning

- PyTorch tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html
"""