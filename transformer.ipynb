{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N8fSfgAa7gWy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer in PyTorch\n",
        "\n",
        "- source: https://github.com/karpathy/nanoGPT"
      ],
      "metadata": {
        "id": "KsVw4zc210vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KiDu9Cp81CG8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Globals"
      ],
      "metadata": {
        "id": "59_oA-_WMQpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    context_len: int = 8\n",
        "    vocab_size: int = 65\n",
        "    n_embd: int = 32\n",
        "    n_head: int = 4\n",
        "    n_layer: int = 3\n",
        "    dropout: float = 0.2\n",
        "    bias: bool = False\n",
        "    flash_attn: bool = False"
      ],
      "metadata": {
        "id": "Q3DLmfoFMXUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Running on device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siPcFf4XMpUG",
        "outputId": "5db4432d-0665-4bd3-8ce8-c67d94630d27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scratch Model"
      ],
      "metadata": {
        "id": "Nb1sbS-J2CO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\" One head of self attention \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.head_size = config.n_embd // config.n_head\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash_attn = config.flash_attn\n",
        "\n",
        "        # One transformation with K,Q,V and all heads combined\n",
        "        self.lin_attn = nn.Linear(config.n_embd, 3 * config.n_embd, config.bias)\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.proj_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Flash attention is fast on GPU but support is only in PyTorch >= 2.0\n",
        "        if not config.flash_attn:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"tril\", torch.tril(torch.ones(config.context_len, config.context_len))\n",
        "                                            .view(1, 1, config.context_len, config.context_len))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        # Calculate k, q, v and reshape to the separate the heads from the key-query space\n",
        "        k, q, v = self.lin_attn(x).split(self.n_embd, dim=2) # (B, T, n_head * head_size)\n",
        "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2) # (B,nh,T,hs)\n",
        "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2) # (B,nh,T,hs)\n",
        "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2) # (B,nh,T,hs)\n",
        "\n",
        "        # Calculate the attention affinities (B,nh,T,hs) x (B,nh,hs,T) = (B,nh,T,T)\n",
        "        if self.flash_attn:\n",
        "            attn = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, droupout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            attn = q @ k.transpose(-2, -1) * self.head_size**-0.5 # (B,nh,T,hs) x (B,nh,hs,T) = (B,nh,T,T)\n",
        "            attn = attn.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf')) # (B,nh,T,T)\n",
        "            attn = F.softmax(attn, dim=-1) # (B,nh,T,T)\n",
        "            attn = self.attn_dropout(attn) # (B,nh,T,T)\n",
        "\n",
        "            # Perform weighted aggregation of values\n",
        "            attn = attn @ v # (B,nh,T,T) @ (B,nh,T,hs) --> (B,nh,T,hs)\n",
        "\n",
        "        out = attn.transpose(1,2).contiguous().view(B, T, C) # Concat the heads\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_dropout(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "FsOM3ohV1nel"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\" Single layer neural network that acts on each token embedding indepentendly \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias) # Inner layer has 4x the embedding size in the paper\n",
        "        self.gelu = nn.GELU()\n",
        "        self.proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias) # Projection back into residual pathway\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6-JpWVlD2X4A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block that alternates attention and computation \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.sa = SelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Note, in original paper, layer norm was applied after the transformations\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "M-R0dLnu2XhZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.context_len is not None\n",
        "        self.config = config\n",
        "\n",
        "        # Each token is embedded into a vector\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        # Add a positional encoding embedding\n",
        "        self.position_embedding_table = nn.Embedding(config.context_len, config.n_embd)\n",
        "        # Dropout for embeddings\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        # Set of transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        # Final layer norm\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # Bring the embeddings back to vocab_size at the output\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.token_embedding_table.weight = self.lm_head.weight\n",
        "\n",
        "        # Init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.3fK\" % (self.get_num_params()/1e3,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.position_embedding_table.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensors of ints\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.context_len, f\"Cannot forward sequence of length {T}, context_len is only {self.config.context_len}\"\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = self.dropout(tok_emb + pos_emb) # (B,T,C)\n",
        "        for block in self.blocks:\n",
        "            x = block(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        # Calculate the cross entropy loss for each example\n",
        "        if targets == None:\n",
        "            # Optimization to only predict the next word for inference\n",
        "            logits = self.lm_head(x[:,[-1],:]) # (B,1,C)\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = self.lm_head(x) # (B,T,C)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # reshape for cross entropy loss\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predicted next tokens\n",
        "            logits, _ = self(idx[:,-self.config.context_len:])\n",
        "            # focus only on the last time step for generation (already done in model)\n",
        "            logits = logits[:, -1, :] / temperature # (B, C)\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to get probabilities of next token\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "            # sample a new token from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sample to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "jYSIKyng2mkV"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Implementation\n",
        "\n",
        "- Additional reference: https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ],
      "metadata": {
        "id": "GkFkSlpS2TBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class TransformerModelPyTorch(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.context_len is not None\n",
        "        self.config = config\n",
        "\n",
        "        # Each token is embedded into a vector\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        # Add a positional encoding embedding\n",
        "        self.position_embedding_table = nn.Embedding(config.context_len, config.n_embd)\n",
        "        # Dropout for embeddings\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        # Set of transformer blocks\n",
        "        decoder_layer = TransformerEncoderLayer(config.n_embd, config.n_head, 4 * config.n_embd, config.dropout, 'gelu', batch_first=True, norm_first=True, bias=config.bias, device=device)\n",
        "        self.transformer_decoder = TransformerEncoder(decoder_layer, config.n_layer)\n",
        "        # Final layer norm\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # Bring the embeddings back to vocab_size at the output\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.token_embedding_table.weight = self.lm_head.weight\n",
        "\n",
        "        # Init all weights\n",
        "        self.init_weights()\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.3fK\" % (self.get_num_params()/1e3,))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.token_embedding_table.weight.data.uniform_(-initrange, initrange)\n",
        "        self.position_embedding_table.weight.data.uniform_(-initrange, initrange)\n",
        "        self.lm_head.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensors of ints\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.context_len, f\"Cannot forward sequence of length {T}, context_len is only {self.config.context_len}\"\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = self.dropout(tok_emb + pos_emb) # (B,T,C)\n",
        "\n",
        "        \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "        Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        src_mask = nn.Transformer.generate_square_subsequent_mask(x.size()[1]).to(device)\n",
        "        x = self.transformer_decoder(x, src_mask, is_causal=True)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        # Calculate the cross entropy loss for each example\n",
        "        if targets == None:\n",
        "            # Optimization to only predict the next word for inference\n",
        "            logits = self.lm_head(x[:,[-1],:]) # (B,1,C)\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = self.lm_head(x) # (B,T,C)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # reshape for cross entropy loss\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.position_embedding_table.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predicted next tokens\n",
        "            logits, _ = self(idx[:,-self.config.context_len:])\n",
        "            # focus only on the last time step for generation (already done in model)\n",
        "            logits = logits[:, -1, :] / temperature # (B, C)\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to get probabilities of next token\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "            # sample a new token from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sample to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "n4qeTyR128On"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Basic - Shakespeare"
      ],
      "metadata": {
        "id": "fQO1SmlW2KVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start with a dataset to train on. Use the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-NhVq3f2E8X",
        "outputId": "04fb8535-7505-4740-842d-cf6cb5afb837"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-05 16:50:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.5’\n",
            "\n",
            "input.txt.5         100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-06-05 16:50:28 (13.5 MB/s) - ‘input.txt.5’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training params\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_iters = 2000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "config = Config()\n",
        "# config.flash_attn = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "print(\"Flash attention enabled: \", config.flash_attn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxAihgbL177g",
        "outputId": "d026e322-fd0a-4c33-9549-12bb74683ded"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device:  cpu\n",
            "Flash attention enabled:  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('input.txt', 'r', encoding='utf-8)') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Define vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create encoder and decoder\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data batch loading\n",
        "def get_batch(split):\n",
        "    # Generate small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - config.context_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+config.context_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+config.context_len+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Estimate the loss from training and validation sets over a number of batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "Ao7kU67k2NTN"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "# Define the model and send to device\n",
        "model = Transformer(config)\n",
        "m = model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # Every once in a while, evaluate the loss on the train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Test model generation\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3stbu4AUG3Z",
        "outputId": "df7819c6-b742-4351-ab45-ce581d31fb47"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 39.168K\n",
            "step 0: train loss 4.1752, val loss 4.1739\n",
            "step 500: train loss 2.5423, val loss 2.5476\n",
            "step 1000: train loss 2.4393, val loss 2.4251\n",
            "step 1500: train loss 2.3876, val loss 2.3827\n",
            "\n",
            "NIRG y byodromns the yaud, avin vkis glond tlond itos kendeicukig, Bre iumrt bemerecpy hyow.\n",
            "\n",
            "\n",
            "ULor afdemou y\n",
            "Ippakrt shah ire htharceweele o afoeaptememl:\n",
            "Thisff I o alyo arit, bratis thabe aot ne herer ithie theallrdeee Cef, whe yazje mon.\n",
            "Goxyurwen aNss bus rort,\n",
            "'nan-Sonalds to kissergavion, and thedt ir eyis an:\n",
            "Thit shi'y cirse tratitich, sfulfacs\n",
            "Gicel ghyogh\n",
            "Reand J kezroul gsre dand dlarror meardtt tthislo thiant is? I ald:\n",
            "Puncow movore, thil: te wis:\n",
            "\n",
            "That ceyd to sre ofrigeen a tislp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "# Define the model and send to device\n",
        "model = TransformerModelPyTorch(config)\n",
        "m = model.to(device)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # Every once in a while, evaluate the loss on the train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "# Test model generation\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfpCwNtE2ev8",
        "outputId": "69b77591-762c-4608-f744-42f659b19114"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 39.168K\n",
            "step 0: train loss 4.2980, val loss 4.2918\n",
            "step 500: train loss 2.5799, val loss 2.5785\n",
            "step 1000: train loss 2.4679, val loss 2.4451\n",
            "step 1500: train loss 2.4312, val loss 2.4040\n",
            "\n",
            "Tat far yeat ho re iprf clomUn ernel.he ancpected mopot'te shos mosor sereaf tope me nold se cerlens! thentll cons?\n",
            "\n",
            "Ass,alt tristhend.\n",
            "\n",
            "\n",
            "MANU:\n",
            "ANur\n",
            "Whiess nore avy,\n",
            "TOTrel lavem.\n",
            "AAURNAng apillo sveity ser anke comy ul bitoecy,\n",
            "An vfum heen?\n",
            "Mit.\n",
            "\n",
            "Shis to ly, Sod sockond Meth sov hes as leyIr tor meri pestes sevilctelldokss h bed al fy.\n",
            "\n",
            "OKThele ospe' ivord;\n",
            "Tneafel helury cusoce: is yoouden? ginerewae, Cerenirst.\n",
            "\n",
            "ILA ININF:\n",
            "peat wemit onergind fyin Hiq thhast Loth notgomit gowroulachey ghird.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Nano GPT"
      ],
      "metadata": {
        "id": "f1sBHCAB7V5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "N8fSfgAa7gWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.context_len is not None\n",
        "        self.config = config\n",
        "\n",
        "        # Each token is embedded into a vector\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        # Add a positional encoding embedding\n",
        "        self.position_embedding_table = nn.Embedding(config.context_len, config.n_embd)\n",
        "        # Dropout for embeddings\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        # Set of transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        # Final layer norm\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
        "        # Bring the embeddings back to vocab_size at the output\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.token_embedding_table.weight = self.lm_head.weight\n",
        "\n",
        "        # Init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.position_embedding_table.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensors of ints\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.context_len, f\"Cannot forward sequence of length {T}, context_len is only {self.config.context_len}\"\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = self.dropout(tok_emb + pos_emb) # (B,T,C)\n",
        "        for block in self.blocks:\n",
        "            x = block(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "\n",
        "        # Calculate the cross entropy loss for each example\n",
        "        if targets == None:\n",
        "            # Optimization to only predict the next word for inference\n",
        "            logits = self.lm_head(x[:,[-1],:]) # (B,1,C)\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = self.lm_head(x) # (B,T,C)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # reshape for cross entropy loss\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predicted next tokens\n",
        "            logits, _ = self(idx[:,-self.config.context_len:])\n",
        "            # focus only on the last time step for generation (already done in model)\n",
        "            logits = logits[:, -1, :] / temperature # (B, C)\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to get probabilities of next token\n",
        "            probs = F.softmax(logits, dim=1) # (B, C)\n",
        "            # sample a new token from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sample to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        \"\"\"\n",
        "        Model surgery to decrease the block size if necessary.\n",
        "        e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        but want to use a smaller block size for some smaller, simpler model\n",
        "        \"\"\"\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.position_embedding_table.weight = nn.Parameter(self.position_embedding_table.weight[:block_size])\n",
        "        for block in self.blocks:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = Config(**config_args)\n",
        "        model = Transformer(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.context_len\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu"
      ],
      "metadata": {
        "id": "7ZRKxmKm7iGd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Full"
      ],
      "metadata": {
        "id": "iHu1Gyk8EbqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This training script can be run both on a single gpu in debug mode,\n",
        "and also in a larger training run with distributed data parallel (ddp).\n",
        "\n",
        "To run on a single GPU, example:\n",
        "$ python train.py --batch_size=32 --compile=False\n",
        "\n",
        "To run with DDP on 4 gpus on 1 node, example:\n",
        "$ torchrun --standalone --nproc_per_node=4 train.py\n",
        "\n",
        "To run with DDP on 4 gpus across 2 nodes, example:\n",
        "- Run on the first (master) node with example IP 123.456.123.456:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "- Run on the worker node:\n",
        "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
        "(If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 500\n",
        "log_interval = 500\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2'\n",
        "\n",
        "# data\n",
        "dataset = 'input.txt' # 'openwebtext'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "\n",
        "# model\n",
        "context_len: int = 8\n",
        "n_embd: int = 32\n",
        "n_head: int = 4\n",
        "n_layer: int = 3\n",
        "dropout: float = 0.2\n",
        "bias: bool = False\n",
        "flash_attn: bool = False\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 1e-3 # max learning rate\n",
        "max_iters = 1000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 500 # how many steps to warm up for\n",
        "lr_decay_iters = max_iters # should be ~= max_iters per Chinchilla\n",
        "min_lr = learning_rate / 10 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster"
      ],
      "metadata": {
        "id": "dJbbSlBCEgBF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * context_len\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nek0TrdX1Qb7",
        "outputId": "0e42a981-7a8d-491c-8d58-409d7983fe01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 3,840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "# def get_batch(split):\n",
        "#     # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "#     # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "#     if split == 'train':\n",
        "#         data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "#     else:\n",
        "#         data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "#     ix = torch.randint(len(data) - context_len, (batch_size,))\n",
        "#     x = torch.stack([torch.from_numpy((data[i:i+context_len]).astype(np.int64)) for i in ix])\n",
        "#     y = torch.stack([torch.from_numpy((data[i+1:i+1+context_len]).astype(np.int64)) for i in ix])\n",
        "#     if device_type == 'cuda':\n",
        "#         # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "#         x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "#     else:\n",
        "#         x, y = x.to(device), y.to(device)\n",
        "#     return x, y\n",
        "\n",
        "with open ('input.txt', 'r', encoding='utf-8)') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# Define vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create encoder and decoder\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data batch loading\n",
        "def get_batch(split):\n",
        "  # Generate small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - context_len, (batch_size,))\n",
        "  x = torch.stack([data[i:i+context_len] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+context_len+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "fKitm6Nd17pX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
      ],
      "metadata": {
        "id": "dhFunMCP2I1J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, context_len=context_len,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout, flash_attn=flash_attn) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(f\"defaulting to vocab_size of GPT-2 to {vocab_size}\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else vocab_size # 50304\n",
        "    config = Config(**model_args)\n",
        "    model = Transformer(config)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'context_len', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    config = Config(**model_args)\n",
        "    model = Transformer(config)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = Transformer.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'context_len', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if context_len < model.config.context_len:\n",
        "    model.crop_context_len(context_len)\n",
        "    model_args['context_len'] = context_len # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sylpyDFz2dQR",
        "outputId": "5d529471-a27c-4e8f-bf92-361b2316a02f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 65\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 0.04M\n",
            "num decayed parameter tensors: 14, with 39,200 parameters\n",
            "num non-decayed parameter tensors: 7, with 224 parameters\n",
            "using fused AdamW: False\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
      ],
      "metadata": {
        "id": "PFrUDAvf2pYd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6u_UQjm231h",
        "outputId": "c049f82f-dc53-423c-b07b-88ad6058df78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1859, val loss 4.1853\n",
            "iter 0: loss 4.1935, time 99632.64ms, mfu -100.00%\n",
            "step 500: train loss 2.4877, val loss 2.4647\n",
            "saving checkpoint to out\n",
            "iter 500: loss 2.3937, time 799.66ms, mfu 0.00%\n",
            "step 1000: train loss 2.3400, val loss 2.3360\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.3475, time 1266.48ms, mfu 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmark"
      ],
      "metadata": {
        "id": "gHHhJgjR9OX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A much shorter version of training for benchmarking\n",
        "\"\"\"\n",
        "import os\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# model\n",
        "context_len: int = 8\n",
        "n_embd: int = 32\n",
        "n_head: int = 4\n",
        "n_layer: int = 3\n",
        "dropout: float = 0.2\n",
        "bias: bool = False\n",
        "flash_attn: bool = False\n",
        "vocab_size = 65\n",
        "\n",
        "real_data = False\n",
        "profile = True\n",
        "seed = 1337\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# data loading init\n",
        "if real_data:\n",
        "    dataset = 'openwebtext'\n",
        "    data_dir = os.path.join('data', dataset)\n",
        "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    def get_batch(split):\n",
        "        data = train_data # note ignore split in benchmarking script\n",
        "        ix = torch.randint(len(data) - context_len, (batch_size,))\n",
        "        x = torch.stack([torch.from_numpy((data[i:i+context_len]).astype(np.int64)) for i in ix])\n",
        "        y = torch.stack([torch.from_numpy((data[i+1:i+1+context_len]).astype(np.int64)) for i in ix])\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "        return x, y\n",
        "else:\n",
        "    # alternatively, if fixed data is desired to not care about data loading\n",
        "    x = torch.randint(vocab_size, (batch_size, context_len), device=device)\n",
        "    y = torch.randint(vocab_size, (batch_size, context_len), device=device)\n",
        "    get_batch = lambda split: (x, y)\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, context_len=context_len,\n",
        "                  bias=bias, vocab_size=vocab_size, dropout=dropout, flash_attn=flash_attn) # start with model_args from command line\n",
        "config = Config(**model_args)\n",
        "model = Transformer(config)\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
        "\n",
        "if compile:\n",
        "    print(\"Compiling model...\")\n",
        "    model = torch.compile(model) # pytorch 2.0\n",
        "\n",
        "if profile:\n",
        "    # useful docs on pytorch profiler:\n",
        "    # - tutorial https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html\n",
        "    # - api https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile\n",
        "    wait, warmup, active = 5, 5, 5\n",
        "    num_steps = wait + warmup + active\n",
        "    with torch.profiler.profile(\n",
        "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
        "        schedule=torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1),\n",
        "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./bench_log'),\n",
        "        record_shapes=False,\n",
        "        profile_memory=False,\n",
        "        with_stack=False, # incurs an additional overhead, disable if not needed\n",
        "        with_flops=True,\n",
        "        with_modules=False, # only for torchscript models atm\n",
        "    ) as prof:\n",
        "\n",
        "        X, Y = get_batch('train')\n",
        "        for k in range(num_steps):\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            X, Y = get_batch('train')\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossf = loss.item()\n",
        "            print(f\"{k}/{num_steps} loss: {lossf:.4f}\")\n",
        "\n",
        "            prof.step() # notify the profiler at end of each step\n",
        "\n",
        "else:\n",
        "\n",
        "    # simple benchmarking\n",
        "    torch.cuda.synchronize()\n",
        "    for stage, num_steps in enumerate([10, 20]): # burnin, then benchmark\n",
        "        t0 = time.time()\n",
        "        X, Y = get_batch('train')\n",
        "        for k in range(num_steps):\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            X, Y = get_batch('train')\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lossf = loss.item()\n",
        "            print(f\"{k}/{num_steps} loss: {lossf:.4f}\")\n",
        "        torch.cuda.synchronize()\n",
        "        t1 = time.time()\n",
        "        dt = t1-t0\n",
        "        mfu = model.estimate_mfu(batch_size * 1 * num_steps, dt)\n",
        "        if stage == 1:\n",
        "            print(f\"time per iteration: {dt/num_steps*1000:.4f}ms, MFU: {mfu*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0fqeKuB2U_f",
        "outputId": "615ae591-e380-49f3-9dbb-30bd151a0cc5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
            "number of parameters: 0.04M\n",
            "num decayed parameter tensors: 14, with 39,200 parameters\n",
            "num non-decayed parameter tensors: 7, with 224 parameters\n",
            "using fused AdamW: False\n",
            "Compiling model...\n",
            "0/15 loss: 4.1906\n",
            "1/15 loss: 4.1784\n",
            "2/15 loss: 4.1711\n",
            "3/15 loss: 4.1773\n",
            "4/15 loss: 4.1589\n",
            "5/15 loss: 4.1610\n",
            "6/15 loss: 4.1455\n",
            "7/15 loss: 4.1403\n",
            "8/15 loss: 4.1380\n",
            "9/15 loss: 4.1252\n",
            "10/15 loss: 4.1281\n",
            "11/15 loss: 4.1289\n",
            "12/15 loss: 4.1158\n",
            "13/15 loss: 4.1070\n",
            "14/15 loss: 4.1037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py:228: UserWarning: CUDA is not available, disabling CUDA profiling\n",
            "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQAtERuz8ibP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}